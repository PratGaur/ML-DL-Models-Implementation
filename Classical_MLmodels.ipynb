{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f9943",
   "metadata": {
    "id": "676f9943"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "# Importing the StringIO module.\n",
    "from io import StringIO\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import random\n",
    "from skimage import color, io\n",
    "import gc\n",
    "import scipy\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30e91",
   "metadata": {
    "id": "33b30e91"
   },
   "outputs": [],
   "source": [
    "class Q1DataLoader():\n",
    "    \n",
    "    def __init__(self,data_path):\n",
    "        self.data = np.load(data_path)\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        x_train = self.data['train_images'].reshape(self.data['train_images'].shape[0],-1)\n",
    "        x_test = self.data['test_images'].reshape(self.data['test_images'].shape[0],-1)\n",
    "        x_val = self.data['val_images'].reshape(self.data['val_images'].shape[0],-1)\n",
    "        y_train = self.data['train_labels']\n",
    "        y_test = self.data['test_labels']\n",
    "        y_val = self.data['val_labels']\n",
    "        \n",
    "        return x_train,y_train,x_test,y_test,x_val,y_val\n",
    "    \n",
    "    def get_metrics(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                if actual[i]==1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "            else:\n",
    "                if actual[i]==1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "\n",
    "        accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "        \n",
    "        if tp+fn==0:\n",
    "            recall= 0 \n",
    "        else:\n",
    "            recall = tp/(tp+fn)\n",
    "        \n",
    "        if tp+fp==0:\n",
    "            precision = 0\n",
    "        else:   \n",
    "            precision = tp/(tp+fp)\n",
    "        \n",
    "        if recall==0 and precision==0:\n",
    "            F1 = 0 \n",
    "        else:\n",
    "            F1 = 2*recall*precision/(recall+precision)\n",
    "\n",
    "        if tn+fp==0:\n",
    "            specificity = 0\n",
    "        else:\n",
    "            specificity = tn/(tn+fp)\n",
    "        \n",
    "        AUC = (recall + specificity)/2\n",
    "\n",
    "        return accuracy,F1,AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fc898",
   "metadata": {
    "id": "b39fc898"
   },
   "outputs": [],
   "source": [
    "class Q2DataLoader():\n",
    "    \n",
    "    def __init__(self,data_path):\n",
    "        self.data = np.load(data_path)\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        x_train = self.data['train_images'].reshape(self.data['train_images'].shape[0],-1)\n",
    "        x_test = self.data['test_images'].reshape(self.data['test_images'].shape[0],-1)\n",
    "        x_val = self.data['val_images'].reshape(self.data['val_images'].shape[0],-1)\n",
    "        y_train = self.data['train_labels']\n",
    "        y_test = self.data['test_labels']\n",
    "        y_val = self.data['val_labels']\n",
    "    \n",
    "        return x_train,y_train,x_test,y_test,x_val,y_val\n",
    "    \n",
    "    def confusion_mat(self,actual, pred):\n",
    "        \n",
    "        classes = 8\n",
    "        mat = np.zeros((8, 8))\n",
    "        for i, j in zip(actual, pred):\n",
    "            mat[i][j] += 1\n",
    "        return mat\n",
    "\n",
    "    def get_metrics(self, y_prediction,y_true):\n",
    "        \n",
    "        unique,counts = np.unique(y_true,return_counts=True)\n",
    "        counts = counts/np.sum(counts)\n",
    "        y_prediction = y_prediction.astype(np.int64)\n",
    "        cnf_matrix = confusion_mat(y_true, y_prediction)\n",
    "        \n",
    "        # print(cnf_matrix)\n",
    "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "        TP = np.diag(cnf_matrix)\n",
    "        TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "        FP = FP.astype(float)\n",
    "        FN = FN.astype(float)\n",
    "        TP = TP.astype(float)\n",
    "        TN = TN.astype(float)\n",
    "        div = TP+FN\n",
    "        TPR = [0 if d==0 else 1/d for d in div]\n",
    "        TPR = TPR * TP\n",
    "\n",
    "        div = TN+FP\n",
    "        TNR = [0 if d==0 else 1/d for d in div]\n",
    "        TNR = TNR * TN\n",
    "\n",
    "        div = TP+FP\n",
    "        PPV = [0 if d==0 else 1/d for d in div]\n",
    "        PPV = PPV * TP\n",
    "\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        AUC = (TNR + TPR)/2\n",
    "        div = TPR + PPV\n",
    "        F1 = [0 if d==0 else 1/d for d in div]\n",
    "        F1 *= 2*TPR*PPV\n",
    "        \n",
    "        f1 = np.sum(F1*counts)\n",
    "        acc = np.sum(ACC*counts)\n",
    "        \n",
    "        return ACC, F1, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb8dc8",
   "metadata": {
    "id": "56bb8dc8"
   },
   "outputs": [],
   "source": [
    "class Q3DataLoader():\n",
    "    \n",
    "    def __init__(self,ann_path,im_path):\n",
    "        \n",
    "        self.ann_path = ann_path\n",
    "        self.im_path = im_path\n",
    "    \n",
    "    def get_file_list(self,root, file_type):\n",
    "        return [os.path.join(directory_path, f) for directory_path, directory_name, \n",
    "            files in os.walk(root) for f in files if f.endswith(file_type)]\n",
    "\n",
    "    def get_train_df(self,ann_path, img_path):\n",
    "    \n",
    "        ann_path_list = self.get_file_list(self.ann_path, '.xml')\n",
    "        ann = np.zeros((len(ann_path_list),4))\n",
    "        for i in range(len(ann_path_list)):\n",
    "            a_path = ann_path_list[i]\n",
    "            root = ET.parse(a_path).getroot()\n",
    "            ann[i][0] = int(root.find(\"./object/bndbox/xmin\").text)\n",
    "            ann[i][1] = int(root.find(\"./object/bndbox/ymin\").text)\n",
    "            ann[i][2] = int(root.find(\"./object/bndbox/xmax\").text)\n",
    "            ann[i][3] = int(root.find(\"./object/bndbox/ymax\").text)\n",
    "        return ann\n",
    "\n",
    "    def get_image_data(self):\n",
    "    \n",
    "        image_list = get_file_list(self.im_path,'png')\n",
    "        image_data = [ cv2.imread(image_path) for image_path in image_list]\n",
    "\n",
    "        return image_data\n",
    "    \n",
    "    def resize_image_bounding_box(self):\n",
    "        \n",
    "        image_data = self.get_image_data()\n",
    "        targetSize = (100,100)\n",
    "        image_list = get_file_list(self.im_path,'png')\n",
    "        resized_image_list = []\n",
    "        for i in range(len(image_data)):\n",
    "            x_scale = 100/image_data[i].shape[0]\n",
    "            y_scale = 100/image_data[i].shape[1]\n",
    "            train_box[i][0] = int(np.round(train_box[i][0]*x_scale))\n",
    "            train_box[i][1] = int(np.round(train_box[i][1]*y_scale))\n",
    "            train_box[i][2] = int(np.round(train_box[i][2]*x_scale))\n",
    "            train_box[i][3] = int(np.round(train_box[i][3]*y_scale))\n",
    "            image = cv2.imread(image_list[i])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ## grayscale image\n",
    "            norm_image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) ## normalize\n",
    "            image = cv2.resize(norm_image,targetSize)  ## resize to 100*100\n",
    "            #print(image.shape)\n",
    "            image = np.array(image)\n",
    "            resized_image_list.append(image.ravel())\n",
    "\n",
    "        return train_box,np.array(resized_image_list)  \n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        train_box,resized_image_list = self.resize_image_bounding_box()\n",
    "        return train_test_split(resized_image_list, train_box, test_size=0.3, random_state=34)\n",
    "    \n",
    "    def getMetrics(self,y_pred,actual):\n",
    "        \n",
    "        xA = max(y_pred[0], actual[0])\n",
    "        yA = max(y_pred[1], actual[1])\n",
    "        xB = min(y_pred[2], actual[2])\n",
    "        yB = min(y_pred[3], actual[3])\n",
    "        # compute the area of intersection rectangle\n",
    "        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "        # compute the area of both the prediction and ground-truth\n",
    "        # rectangles\n",
    "        boxAArea = (y_pred[2] - y_pred[0] + 1) * (y_pred[3] - y_pred[1] + 1)\n",
    "        boxBArea = (actual[2] - actual[0] + 1) * (actual[3] - actual[1] + 1)\n",
    "        # compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        # areas - the interesection area\n",
    "        iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "        # return the intersection over union value\n",
    "\n",
    "        MSE = np.mean(0.5 * (actual - y_pred)**2)\n",
    "        MAE = np.mean(np.abs(actual - y_pred))\n",
    "        \n",
    "        return MSE,MoU/y_pred.shape[0],MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3a72a",
   "metadata": {
    "id": "48f3a72a"
   },
   "outputs": [],
   "source": [
    "class Q4DataLoader():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.phoneme = {}\n",
    "        phoneme_list = [\"bcl\",\"dcl\",\"gcl\",\"pcl\",\"tck\",\"kcl\",\"dcl\",\"tcl\",\"b\",\"d\",\"g\",\"p\",\"t\",\"k\",\"dx\",\"q\",\"jh\",\"ch\",\"s\",\"sh\",\"z\",\"zh\",\"f\",\"th\",\"v\",\"dh\",\"m\",\"n\",\"ng\",\"em\",\"en\",\"eng\",\"nx\",\"l\",\"r\",\"w\",\"y\",\"hh\",\"hv\",\"el\",\"iy\",\"ih\",\"eh\",\"ey\",\"ae\",\"aa\",\"aw\",\"ay\",\"ah\",\"ao\",\"oy\",\"ow\",\"uh\",\"uw\",\"ux\",\"er\",\"ax\",\"ix\",\"axr\",\"ax-h\",\"pau\",\"epi\",\"h#\"]\n",
    "        phonemlist_length = 63\n",
    "        #create key value dictionary\n",
    "        for ph in phoneme_list: \n",
    "            if('a' in ph or 'e' in ph or 'i' in ph or 'o' in ph or 'u' in ph):\n",
    "                self.phoneme[ph] = 0                                                ##phoneme is vowel\n",
    "            else:\n",
    "                self.phoneme[ph] = 1\n",
    "    \n",
    "    def get_max_feature_len(self,x):\n",
    "        \n",
    "        max_len = 0\n",
    "        n=x.__len__()\n",
    "        for i in range(n):\n",
    "            max_len = max(max_len,x[i].__len__())\n",
    "        return max_len\n",
    "    \n",
    "    def add_padding(self,x,max_len):\n",
    "    \n",
    "        x_train = []\n",
    "        n = x.__len__()\n",
    "        for i in range(n):\n",
    "            m=x[i].__len__()\n",
    "            temp=np.zeros(max_len)\n",
    "            if(m>max_len):\n",
    "                temp=x[i][:max_len]\n",
    "            else:\n",
    "                temp[:m]=x[i]\n",
    "\n",
    "            x_train.append(temp)\n",
    "        return x_train\n",
    "\n",
    "    def get_x_and_y(self,file_path):\n",
    "    \n",
    "        x = []\n",
    "        y = []\n",
    "        count = 0\n",
    "        for folder in os.listdir(file_path):\n",
    "            \n",
    "            path = file_path + folder + \"/\"\n",
    "            temp_name = \"\"\n",
    "            for files in os.listdir(path):\n",
    "                name = files.split(\".\")[0]\n",
    "                if name != temp_name:\n",
    "                    temp_name = name\n",
    "                wav_file = path + name + \".WAV\"\n",
    "                phn_file = path + name + \".PHN\"\n",
    "\n",
    "                data, sampling_freq = librosa.load(wav_file,sr=None, mono=True,offset=0.0,duration=None)\n",
    "                data=data.tolist()\n",
    "\n",
    "                file_obj = open(phn_file, 'r')\n",
    "                phonem_data = file_obj.readlines()\n",
    "                n=np.shape(phonem_data)[0]\n",
    "\n",
    "                for i in range(n):\n",
    "                    \n",
    "                    lower,upper,ph=phonem_data[i].split(\" \")\n",
    "                    lower = int(lower)\n",
    "                    upper = int(upper)\n",
    "                    ph = ph.replace(\"\\n\", \"\")\n",
    "                    temp = data[lower:upper]\n",
    "                    temp = np.array(temp)\n",
    "                    count += 1\n",
    "                    mfccs = librosa.feature.mfcc(temp, sr=sampling_freq)\n",
    "                    mfccs = mfccs.flatten()\n",
    "                    temp = mfccs.tolist()\n",
    "                    x.append(temp)\n",
    "                    y.append(self.phoneme[ph])\n",
    "                    \n",
    "        return x,y\n",
    "    \n",
    "    def preprocessing_audio(self,path):\n",
    "  \n",
    "        path_test  = path + \"test/DR1/\"\n",
    "        path_train = path + \"train/DR1/\"\n",
    "        x_train,y_train = self.get_x_and_y(path_train)         \n",
    "        x_test,y_test = self.get_x_and_y(path_test)\n",
    "        max_len = self.get_max_feature_len(x_train)\n",
    "        max_len = max(max_len,self.get_max_feature_len(x_test))\n",
    "        x_train = self.add_padding(x_train,max_len)\n",
    "        x_test = self.add_padding(x_test,max_len)\n",
    "\n",
    "        return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "    \n",
    "    def get_metrics(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                if actual[i]==1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "            else:\n",
    "                if actual[i]==1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "        \n",
    "        return tp/actual.shape[0],fp/actual.shape[0],tn/actual.shape[0],fn/actual.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W74J85k3ghXp",
   "metadata": {
    "id": "W74J85k3ghXp"
   },
   "outputs": [],
   "source": [
    "class Q5DataLoader():\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with ZipFile(path, 'r') as zipObj:\n",
    "            zipObj.extractall()\n",
    "\n",
    "        self.test_path = '/content/tiny-imagenet-200/test'\n",
    "        self.train_path = '/content/tiny-imagenet-200/train'\n",
    "        self.validation_path = '/content/tiny-imagenet-200/val'\n",
    "        \n",
    "    \n",
    "    def extract(self):\n",
    "        x = []\n",
    "        NO_CLASSES = 10\n",
    "        NO_IMAGES = 100\n",
    "        class_folders = os.listdir(self.train_path)\n",
    "\n",
    "        # loop over classes\n",
    "        for class_folder in class_folders[:NO_CLASSES]: \n",
    "\n",
    "        image_folder_path = train_path + os.path.sep + class_folder + os.path.sep + 'images' + os.path.sep\n",
    "        image_file_names = os.listdir(image_folder_path)\n",
    "\n",
    "        # loop over images\n",
    "        for image_file_name in image_file_names[:NO_IMAGES]: # loop over all images in a class\n",
    "            img = io.imread(image_folder_path + image_file_name)\n",
    "            img = cv2.resize(img, (28, 28))\n",
    "            if len(img.shape) == 3:\n",
    "                imgGray = 0.3*img[:,:,0] + 0.59*img[:,:,1] + 0.11*img[:,:,2]\n",
    "                flat_image = imgGray.flatten() \n",
    "            else:\n",
    "                flat_image = img.flatten()\n",
    "            x.append(flat_image)\n",
    "        X = np.array(x) / 255\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c022d4",
   "metadata": {
    "id": "e8c022d4"
   },
   "outputs": [],
   "source": [
    "class Experiments():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def binary_experiments(self,X,y,x_test,y_test,counts,dataset):\n",
    "        \n",
    "        clf = GaussianMAP(n_classes=2)\n",
    "        clf.fit(X,counts)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of my Gaussian MLE = {accuracy,F1,AUC}') \n",
    "            \n",
    "        clf = LogisticRegression('Elastic',0.001,0.1)\n",
    "        clf.fit(X,y)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of Elastic Net Logistic Regression = {accuracy,F1,AUC}')\n",
    "        clf.plots()\n",
    "\n",
    "        clf = LogisticRegression(None,None,None)\n",
    "        clf.fit(X,y)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of L1 Logistic Regression = {accuracy,F1,AUC}')\n",
    "        clf.plots()\n",
    "\n",
    "        clf = LogisticRegression('L1',0.001,None)\n",
    "        clf.fit(X,y)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of L1 Logistic Regression = {accuracy,F1,AUC}')\n",
    "        clf.plots()\n",
    "\n",
    "        clf = LogisticRegression('L2',0.001,None)\n",
    "        clf.fit(X,y)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of L2 Logistic Regression = {accuracy,F1,AUC}')\n",
    "        clf.plots()\n",
    "\n",
    "        clf = LDA(n_classes=2,n_features = x_train.shape[1])\n",
    "        clf.fit(X,counts)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of LDA = {accuracy,F1,AUC}') \n",
    "\n",
    "        clf = Multiclass_Gaussian_Naive_Bayes(x_train.shape[1],2)\n",
    "        clf.fit(X,counts)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of Multiclass Gaussian Naive Bayes = {accuracy,F1,AUC}') \n",
    "\n",
    "        clf = KNearestNeighbor(X,y,10)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of K nearest neighbor = {accuracy,F1,AUC}') \n",
    "\n",
    "        clf = GaussianMAP(n_classes=2)\n",
    "        clf.fit(X,counts)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of my Gaussian MLE = {accuracy,F1,AUC}') \n",
    "\n",
    "        clf = ParzenWindow(X,n_classes = 2)\n",
    "        pred = clf.predict(x_test,counts)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of Parzen Window = {accuracy,F1,AUC}') \n",
    "        \n",
    "        clf = GMM_classification(n_classes=2,n_clusters=3)\n",
    "        pred = clf.fit_predict(X,x_test,counts)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'accuracy of GMM with 3 clusters per class = {accuracy,F1,AUC}')\n",
    "        \n",
    "    def multi_class_experiments(self,X,y,x_test,y_test,counts,dataset):\n",
    "        \n",
    "        clf = Multiclass_Logistic_Regression(None,None,None)\n",
    "        clf.fit(X,y)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of  Logistic Regression = {accuracy,F1,AUC}')\n",
    "        clf.plots()\n",
    "        \n",
    "        clf = Multiclass_Logistic_Regression('Elastic',0.1,0.5)\n",
    "        clf.fit(X,y)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of Elastic Net Logistic Regression  = {accuracy,F1,AUC}')\n",
    "        clf.plots()\n",
    "\n",
    "        clf = Multiclass_Logistic_Regression('L1',0.1,None)\n",
    "        clf.fit(X,y)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of L1 Logistic Regression  = {accuracy,F1,AUC}')\n",
    "        clf.plots()\n",
    "\n",
    "        clf = Multiclass_Logistic_Regression('L2',0.1,None)\n",
    "        clf.fit(X,y)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of L2 Logistic Regression = {accuracy,F1,AUC}')\n",
    "        clf.plots()\n",
    "        \n",
    "        clf = Multiclass_Gaussian_Naive_Bayes(x_train.shape[1],8)\n",
    "        clf.fit(X,counts)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of Multiclass Gaussian Naive Bayes = {accuracy,F1,AUC}') \n",
    "\n",
    "        clf = Multi_LDA(n_classes=8,n_features = x_train.shape[1])\n",
    "        clf.fit(X,counts)\n",
    "        pred = clf.predict(x_test,n_components = 7)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of Multiclass LDA = {accuracy,F1,AUC}') \n",
    "\n",
    "        clf = ParzenWindow(X,n_classes = 8)\n",
    "        pred = clf.predict(x_test,counts)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of Parzen Window = {accuracy,F1,AUC}')\n",
    "\n",
    "        clf = KNearestNeighbor(X,y,1)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of K nearest neighbor = {accuracy,F1,AUC}') \n",
    "\n",
    "        clf = GaussianMLE(n_classes=8)\n",
    "        clf.fit(X,counts)\n",
    "        pred = clf.predict(x_test)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of Gaussian MLE = {accuracy,F1,AUC}') \n",
    "    \n",
    "        clf = GMM_classification(n_classes = 8,n_clusters=3)\n",
    "        pred = clf.fit_predict(X,x_test,counts)\n",
    "        accuracy,F1,AUC = dataset.get_metrics(pred,y_test)\n",
    "        print(f'metrics of GMM classification = {accuracy,F1,AUC}')\n",
    "    \n",
    "    def Regression_experiments(self,X,y,x_test,y_test,dataset):\n",
    "        \n",
    "        clf = LinearRegression(1e-7,None,None,None)\n",
    "        clf.fit(X_train,y_train)\n",
    "        training_errors = np.array(clf.training_errors)\n",
    "        training_MAE = np.array(clf.MAE)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        MSE,mIoU,MAE = dataset.getMetrics(y_pred,y_test) \n",
    "        print(f'metrics for Simple Linear are {MSE,mIoU,MAE}')\n",
    "        clf.plot_loss()\n",
    "\n",
    "        clf = LinearRegression(1e-7,0.1,'L1',None)\n",
    "        clf.fit(X_train,y_train)\n",
    "        training_errors = np.array(clf.training_errors)\n",
    "        training_MAE = np.array(clf.MAE)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        MSE,mIoU,MAE = dataset.getMetrics(y_pred,y_test) \n",
    "        print(f'metrics for L1 are {MSE,mIoU,MAE}')\n",
    "        clf.plot_loss()\n",
    "\n",
    "        clf = LinearRegression(1e-7,0.1,'L2',None)\n",
    "        clf.fit(X_train,y_train)\n",
    "        training_errors = np.array(clf.training_errors)\n",
    "        training_MAE = np.array(clf.MAE)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        MSE,mIoU,MAE = dataset.getMetrics(y_pred,y_test) \n",
    "        print(f'metrics for L2 are {MSE,mIoU,MAE}')\n",
    "        clf.plot_loss()\n",
    "\n",
    "        clf = LinearRegression(1e-7,0.1,'Elastic',0.1)\n",
    "        clf.fit(X_train,y_train)\n",
    "        training_errors = np.array(clf.training_errors)\n",
    "        training_MAE = np.array(clf.MAE)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        MSE,mIoU,MAE = dataset.getMetrics(y_pred,y_test) \n",
    "        print(f'metrics for Elastic are {MSE,mIoU,MAE}')\n",
    "        clf.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b23a7",
   "metadata": {
    "id": "cf3b23a7"
   },
   "outputs": [],
   "source": [
    "class Multiclass_Gaussian_Naive_Bayes():\n",
    "    \n",
    "    def __init__(self,n_features,n_classes):\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.mean = np.zeros((n_classes,n_features))\n",
    "        self.variance = np.zeros((n_classes,n_features))\n",
    "        self.eps = 1e-7\n",
    "        \n",
    "    def fit(self,X,counts):\n",
    "    \n",
    "        for i in range(np.size(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            self.mean[i] = np.mean(curr_X,axis = 0)\n",
    "            diff = curr_X - self.mean[i]\n",
    "            self.variance[i] = np.mean(np.square(diff),axis = 0)\n",
    "\n",
    "    def gaussian(self,X, mu, var):\n",
    "        \n",
    "        return 1 / ((2 * np.pi) ** (1 / 2) * var ** 0.5) * np.exp(-0.5 * (X-mu)**2/var)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        \n",
    "        for j in range(x_test.shape[0]):\n",
    "            best_class = 0\n",
    "            best_likelihood = -math.inf\n",
    "            for i in range(self.n_classes):\n",
    "                likelihood = self.gaussian(x_test[j],self.mean[i],self.variance[i])\n",
    "                log_likelihood = np.sum(np.log(likelihood+self.eps))\n",
    "                if best_likelihood < log_likelihood:\n",
    "                    best_likelihood = log_likelihood\n",
    "                    best_class = i\n",
    "            pred[j] = best_class\n",
    "        return pred\n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1792d",
   "metadata": {
    "id": "0ab1792d"
   },
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    \n",
    "    def __init__(self,n_features,n_classes):\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.S_w = np.zeros((n_features,n_features))\n",
    "        self.S_b = np.zeros((n_features,n_features))\n",
    "        self.mu = np.zeros(n_features)\n",
    "        self.mu_class = np.zeros((self.n_classes,self.n_features))\n",
    "\n",
    "    def fit(self,X,counts):\n",
    "        \n",
    "        self.mu = np.mean(X,axis = 0)\n",
    "        self.mu_class[0] = np.mean(X[0:counts[0],:],axis = 0)\n",
    "        self.mu_class[1] = np.mean(X[counts[0]:,:],axis = 0)\n",
    "        self.S_w = np.dot((X[0:counts[0],:]-self.mu_class[0]).T,X[0:counts[0],:]-self.mu_class[0]) + np.dot((X[counts[0]:,:]-self.mu_class[1]).T,X[counts[0]:,:]-self.mu_class[1])\n",
    "        self.S_b = np.dot((self.mu_class[0] - self.mu_class[1]).T,self.mu_class[0] - self.mu_class[1])\n",
    "            \n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        v = np.dot(np.linalg.pinv(self.S_w),self.mu_class[0] - self.mu_class[1])\n",
    "        pred = [1 if abs(np.dot(x_test[i],v)-np.dot(self.mu_class[0],v))> abs(np.dot(x_test[i],v)-np.dot(self.mu_class[1],v)) else 0 for i in range(x_test.shape[0])]\n",
    "        return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac8fb6",
   "metadata": {
    "id": "61ac8fb6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LinearRegression():\n",
    "    \n",
    "    def __init__(self, learning_rate,alpha,regularization,L1_ratio):\n",
    "            \n",
    "        self.param = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_rate = alpha\n",
    "        self.regularization = regularization\n",
    "        self.L1_ratio = L1_ratio\n",
    "        self.n_iterations = 2000\n",
    "    def initialize_parameters(self, input_dim,output_dim):\n",
    "        \n",
    "        self.param = np.ones((input_dim,output_dim))\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        l1_ratio = self.L1_ratio\n",
    "        x = np.insert(x, 0, 1, axis=1)\n",
    "        self.training_errors = []\n",
    "        self.MAE = []\n",
    "        self.initialize_parameters(input_dim = x.shape[1],output_dim = y.shape[1])\n",
    "        print(self.param.shape)\n",
    "        # Do gradient descent for n_iterations\n",
    "        for i in tqdm(range(self.n_iterations)):\n",
    "            \n",
    "            self.learning_rate = self.learning_rate/(10**int((i/1000))) ## learning rate decay\n",
    "            y_pred = np.dot(x,self.param)\n",
    "            if self.regularization == None:\n",
    "                mse = np.mean(0.5 * (y - y_pred)**2)\n",
    "                grad_param = np.dot(x.T,(y_pred - y))\n",
    "            elif self.regularization == 'L1':\n",
    "                mse = np.mean(0.5 * (y - y_pred)**2) + self.reg_rate*np.linalg.norm(self.param, ord= 1)\n",
    "                grad_param = np.dot(x.T,(y_pred - y)) + self.reg_rate*np.sign(self.param)\n",
    "            elif self.regularization == 'L2':\n",
    "                mse = np.mean(0.5 * (y - y_pred)**2) + 0.5*self.reg_rate*np.linalg.norm(self.param)**2\n",
    "                grad_param = np.dot(x.T,(y_pred - y)) + self.reg_rate*self.param\n",
    "            elif self.regularization == 'Elastic':\n",
    "                mse = np.mean(0.5 * (y - y_pred)**2) + self.reg_rate*(l1_ratio*np.linalg.norm(self.param, ord= 1)+(1-l1_ratio)*0.5*np.linalg.norm(self.param)**2)\n",
    "                grad_param = np.dot(x.T,(y_pred - y)) + self.reg_rate*((1-l1_ratio)*self.param + l1_ratio*np.sign(self.param))\n",
    "            \n",
    "            self.training_errors.append(mse)\n",
    "            self.MAE.append(np.mean(np.abs(y-y_pred)))\n",
    "            self.param = self.param - self.learning_rate * grad_param\n",
    "        \n",
    "            \n",
    "    def predict(self,x_test):\n",
    "        x_test = np.insert(x_test, 0, 1, axis=1)\n",
    "        pred = np.dot(x_test,self.param)\n",
    "        return pred\n",
    "    \n",
    "    def plot_loss(self):\n",
    "    \n",
    "        iters = np.arange(0,self.n_iterations,1)\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        plt.plot(iters[50:],self.training_errors[50:],label = 'MSE')\n",
    "        plt.plot(iters[50:],self.MAE[50:],label = 'MAE')\n",
    "        plt.xlabel('No. of iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        if self.regularization == None:\n",
    "            plt.title('Simple Linear Regression')\n",
    "            plt.savefig('Q3_LinearRegression.png')\n",
    "        else:\n",
    "            plt.title(f'{self.regularization}Regression')\n",
    "            plt.savefig(f'Q3{self.regularization}.png')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d657d",
   "metadata": {
    "id": "731d657d"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self,regularization,reg_rate,l1_ratio,learning_rate=0.001):\n",
    "        self.param = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = []\n",
    "        self.eps = 1e-7\n",
    "        self.n_iterations = 2000\n",
    "        self.regularization = regularization\n",
    "        self.reg_rate = reg_rate\n",
    "        self.l1_ratio = l1_ratio\n",
    "        \n",
    "    def initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        self.param = np.ones((n_features,1))\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X,0,1,axis = 1)\n",
    "        y = y.reshape(-1,1)\n",
    "        self.initialize_parameters(X)\n",
    "        l1_ratio = self.l1_ratio\n",
    "        for i in tqdm(range(self.n_iterations)):\n",
    "            \n",
    "            y_pred = self.sigmoid(np.dot(X,self.param))\n",
    "            error = -np.dot(y.T,np.log(y_pred + self.eps)).item()\n",
    "            if self.regularization==None:\n",
    "                self.param -= self.learning_rate * np.dot(X.T,y_pred - y)\n",
    "            elif self.regularization == 'L1':\n",
    "                self.param -= self.learning_rate * (np.dot(X.T,y_pred - y) + self.reg_rate*np.sign(self.param))\n",
    "            elif self.regularization == 'L2':\n",
    "                self.param -= self.learning_rate * (np.dot(X.T,y_pred - y) + self.reg_rate*self.param)\n",
    "            elif self.regularization == 'Elastic':\n",
    "                self.param -= self.learning_rate * (np.dot(X.T,y_pred - y) + self.reg_rate*((1-l1_ratio)*self.param + l1_ratio*np.sign(self.param)))\n",
    "    \n",
    "            self.loss.append(error)                                     \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis = 1)\n",
    "        y_pred = np.round(self.sigmoid(np.dot(X,self.param))).astype(int)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def plots(self):\n",
    "        \n",
    "        iters = np.arange(0,self.n_iterations,1)\n",
    "        fig = plt.figure()\n",
    "        plt.plot(iters,self.loss)\n",
    "        plt.xlabel('No. of iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        if self.regularization == None:\n",
    "            plt.title('Simple Logistic Regression')\n",
    "            plt.savefig('Q1_LogisticRegression.png')\n",
    "        else:\n",
    "            plt.title(f'{self.regularization}_LogisticRegression')\n",
    "            plt.savefig(f'Q1{self.regularization}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff87dd4",
   "metadata": {
    "id": "7ff87dd4"
   },
   "outputs": [],
   "source": [
    "class ParzenWindow():\n",
    "    def __init__(self,X_train,n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.x_train = X_train\n",
    "        \n",
    "    def predict(self,X_test,counts):\n",
    "        \n",
    "        y_pred = np.zeros((self.n_classes,X_test.shape[0]))\n",
    "        for i in range(np.size(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "                \n",
    "            curr_X = self.x_train[st_idx:st_idx+counts[i]]\n",
    "            y_pred[i,:] = np.array([np.sum(np.exp(-1*((np.linalg.norm(curr_X - X_test[idx],axis=1))**2))) for idx in range(X_test.shape[0])])\n",
    "        \n",
    "        return np.argmax(y_pred,axis=0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3a845",
   "metadata": {
    "id": "d8e3a845"
   },
   "outputs": [],
   "source": [
    "class Multiclass_Logistic_Regression():\n",
    "    \n",
    "    def __init__(self, regularization,reg_rate,l1_ratio,learning_rate=0.001,n_classes=8):\n",
    "        \n",
    "        self.param = None\n",
    "        self.lr = learning_rate\n",
    "        self.training_errors = []\n",
    "        self.eps = 1e-7\n",
    "        self.n_classes = n_classes\n",
    "        self.regularization = regularization\n",
    "        self.reg_rate = reg_rate\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.n_iterations=1000\n",
    "        \n",
    "    def initialize_parameters(self, X):\n",
    "        \n",
    "        n_features = np.shape(X)[1]\n",
    "        self.param = np.ones((self.n_classes,n_features))\n",
    "    \n",
    "    def one_hot(self,y):\n",
    "\n",
    "        return np.eye(self.n_classes)[y.reshape(-1)]\n",
    "    \n",
    "    def softmax(self,probs):\n",
    "        probs = probs - (np.mean(probs,axis=1)).reshape(-1,1)  \n",
    "        return np.exp(probs)/(np.sum(np.exp(probs),axis = 1) + self.eps).reshape(-1,1)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        y = self.one_hot(y)\n",
    "        self.initialize_parameters(X)\n",
    "        l1_ratio = self.l1_ratio\n",
    "        for i in tqdm(range(self.n_iterations)):\n",
    "            y_pred = softmax(np.dot(X,self.param.T),axis = 1)\n",
    "            if self.regularization==None:\n",
    "                loss = -1*np.mean(y*np.log(y_pred + self.eps))\n",
    "                grad = np.dot((y_pred-y).T,X)\n",
    "            elif self.regularization == 'L1':\n",
    "                loss = -1*np.mean(y*np.log(y_pred + self.eps)) + self.reg_rate*np.linalg.norm(self.param, ord= 1)\n",
    "                grad =np.dot((y_pred-y).T,X) + self.reg_rate*np.sign(self.param)\n",
    "            elif self.regularization == 'L2':\n",
    "                loss = -1*np.mean(y*np.log(y_pred + self.eps)) + 0.5*self.reg_rate*np.linalg.norm(self.param)**2\n",
    "                grad = np.dot((y_pred-y).T,X) + self.reg_rate*self.param\n",
    "            elif self.regularization == 'Elastic':\n",
    "                loss = -1*np.mean(y*np.log(y_pred + self.eps)) + self.reg_rate*(l1_ratio*np.linalg.norm(self.param, ord= 1)+(1-l1_ratio)*0.5*np.linalg.norm(self.param)**2)\n",
    "                grad = np.dot((y_pred-y).T,X) + self.reg_rate*((1-l1_ratio)*self.param + l1_ratio*np.sign(self.param))\n",
    "            self.training_errors.append(loss)\n",
    "            self.param = self.param - self.lr*grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        y_pred = np.argmax(softmax(np.dot(X,self.param.T),axis=1),axis = 1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def plots(self):\n",
    "        \n",
    "        iters = np.arange(0,self.n_iterations,1)\n",
    "        fig = plt.figure()\n",
    "        plt.plot(iters,self.training_errors)\n",
    "        plt.xlabel('No. of iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        if self.regularization == None:\n",
    "            plt.title('Simple Logistic Regression')\n",
    "            plt.savefig('Q2_LogisticRegression.png')\n",
    "        else:\n",
    "            plt.title(f'{self.regularization}_LogisticRegression')\n",
    "            plt.savefig(f'Q2{self.regularization}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8353a0a",
   "metadata": {
    "id": "b8353a0a"
   },
   "outputs": [],
   "source": [
    "class GaussianMAP():\n",
    "    \n",
    "    def __init__(self,n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.eps = 1e-7\n",
    "    \n",
    "    def fit(self,X,counts):\n",
    "        \n",
    "        self.mu = np.zeros((self.n_classes,X.shape[1]))\n",
    "        self.cov = np.zeros((self.n_classes,X.shape[1],X.shape[1]))\n",
    "        self.log_prior = np.log(counts/np.sum(counts))\n",
    "        \n",
    "        for i in range(len(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            self.mu[i] = np.mean(curr_X,axis = 0)\n",
    "            self.cov[i] = np.dot((curr_X-self.mu[i]).T,curr_X-self.mu[i])\n",
    "            \n",
    "    def log_likelihood(self,X, mu, cov):\n",
    "        \n",
    "        sign,log_det = np.linalg.slogdet(cov)\n",
    "        return -0.5*log_det -0.5*np.dot(np.dot((X-mu).T,np.linalg.pinv(cov)),X-mu)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in tqdm(range(x_test.shape[0])):\n",
    "            best_class = 0\n",
    "            best_posterior = -math.inf\n",
    "            for j in range(self.n_classes):\n",
    "                log_likelihood = self.log_likelihood(x_test[i],self.mu[j],self.cov[j])\n",
    "                log_posterior = log_likelihood + self.log_prior[j]\n",
    "                if best_posterior < log_posterior:\n",
    "                    best_posterior = log_posterior\n",
    "                    best_class = j\n",
    "            pred[i] = best_class\n",
    "        return pred\n",
    "    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558eed81",
   "metadata": {
    "id": "558eed81"
   },
   "outputs": [],
   "source": [
    "class Multi_LDA():\n",
    "\n",
    "    def __init__(self,n_features,n_classes):\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.S_w = np.zeros((n_features,n_features))\n",
    "        self.S_b = np.zeros((n_features,n_features))\n",
    "        self.mu = np.zeros(n_features)\n",
    "        self.mu_class = np.zeros((self.n_classes,self.n_features))\n",
    "\n",
    "    def fit(self,X,counts):\n",
    "        \n",
    "        self.mu = np.mean(X,axis = 0)\n",
    "        for i in range(self.n_classes):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            self.mu_class[i] = np.mean(curr_X,axis = 0)\n",
    "            self.S_w = self.S_w + np.dot((curr_X - self.mu_class[i]).T,curr_X - self.mu_class[i])\n",
    "            self.S_b = self.S_b + counts[i]*np.dot((self.mu - self.mu_class[i]).T,self.mu - self.mu_class[i])\n",
    "     \n",
    "    def predict(self,x_test,n_components):\n",
    "        \n",
    "        V = np.dot(np.linalg.pinv(self.S_w),self.S_b)\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(V)\n",
    "        eigenvectors = eigenvectors.T\n",
    "        eigenList = [(eigenvalues[i],eigenvectors[i,:]) for i in range(len(eigenvalues))]\n",
    "        eigenList = sorted(eigenList,key = lambda x:x[0] ,reverse= True)\n",
    "        for i in range(n_components):\n",
    "            eigenvectors[i,:] = eigenList[i][1]\n",
    "        # Project the data onto eigenvectors\n",
    "        eigenvectors = eigenvectors[0:n_components,:]\n",
    "        projected_X = np.dot(x_test,eigenvectors.T)\n",
    "        print(eigenvectors.shape,projected_X.shape)\n",
    "        projected_mu = np.dot(self.mu_class,eigenvectors.T)\n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in range(x_test.shape[0]):\n",
    "            best_class = 0\n",
    "            best_dist = math.inf\n",
    "            for j in range(self.n_classes):\n",
    "                if np.linalg.norm(projected_X[i] - projected_mu[j]) < best_dist:\n",
    "                    best_dist = np.linalg.norm(projected_X[i] - projected_mu[j])\n",
    "                    best_class = j\n",
    "            pred[i] = best_class\n",
    "        \n",
    "        return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09069b8e",
   "metadata": {
    "id": "09069b8e"
   },
   "outputs": [],
   "source": [
    "class KNearestNeighbor():\n",
    "    \n",
    "    def __init__(self,x_train,y_train,K):\n",
    "        self.k = K\n",
    "        self.X = x_train\n",
    "        self.y = y_train\n",
    "    \n",
    "    def get_Euclidean_distance(self,x):\n",
    "        \n",
    "        return np.sqrt(np.sum((self.X - x)**2,axis=1))\n",
    "        \n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in range(x_test.shape[0]):\n",
    "            dist = self.get_Euclidean_distance(x_test[i])\n",
    "            nearest_neighbors = dist.argsort()[0:self.k]\n",
    "            unique,counts = np.unique(self.y[nearest_neighbors], return_counts = True)\n",
    "            pred[i] = unique[np.argmax(counts)]\n",
    "            \n",
    "        return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213e961",
   "metadata": {
    "id": "e213e961"
   },
   "outputs": [],
   "source": [
    "class GMM():\n",
    "\n",
    "    def __init__(self, n_components, n_iters, tol):\n",
    "        self.n_components = n_components\n",
    "        self.n_iters = n_iters\n",
    "        self.tol = tol\n",
    "        self.eps = 1e-7\n",
    "        \n",
    "    def fit(self, X):\n",
    "\n",
    "        # data's dimensionality and responsibility vector\n",
    "        n_row, n_col = X.shape     \n",
    "        self.resp = np.zeros((n_row, self.n_components))\n",
    "        \n",
    "        # initialize parameters\n",
    "        np.random.seed(4)\n",
    "        chosen = np.random.choice(n_row, self.n_components, replace = False)\n",
    "        self.means = X[chosen]\n",
    "        self.weights = np.full(self.n_components, 1 / self.n_components)\n",
    "\n",
    "        shape = self.n_components, n_col, n_col\n",
    "        self.covs = np.full(shape, np.cov(X, rowvar = False))\n",
    "\n",
    "        log_likelihood = 0\n",
    "        self.converged = False\n",
    "        self.log_likelihood_trace = []      \n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            log_likelihood_new = self.Expectation(X)\n",
    "            self.Maximization(X)\n",
    "\n",
    "            if abs(log_likelihood_new - log_likelihood) <= self.tol:\n",
    "                self.converged = True\n",
    "                break\n",
    "  \n",
    "            log_likelihood = log_likelihood_new\n",
    "            self.log_likelihood_trace.append(log_likelihood)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def Expectation(self, X):\n",
    "        \n",
    "        self._compute_log_likelihood(X)\n",
    "        log_likelihood = np.sum(np.log(np.sum(self.resp, axis = 1)+ self.eps) )\n",
    "        self.resp = self.resp / (self.resp.sum(axis = 1, keepdims = 1) + self.eps)\n",
    "        \n",
    "        return log_likelihood\n",
    "\n",
    "    def _compute_log_likelihood(self, X):\n",
    "        self.reg_cov = 1e-6*np.identity(X.shape[1])\n",
    "        assert(np.shape(self.covs[0])==np.shape(self.reg_cov))\n",
    "        for k in range(self.n_components):\n",
    "            prior = self.weights[k]\n",
    "            likelihood = multivariate_normal(self.means[k], self.covs[k]+self.reg_cov).pdf(X)\n",
    "            self.resp[:, k] = prior * likelihood\n",
    "\n",
    "        return self\n",
    "\n",
    "    def Maximization(self, X):\n",
    "\n",
    "        resp_weights = self.resp.sum(axis = 0)\n",
    "        self.weights = resp_weights / X.shape[0]\n",
    "        weighted_sum = np.dot(self.resp.T, X)\n",
    "        self.means = weighted_sum / (resp_weights.reshape(-1, 1) + self.eps)\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            diff = (X - self.means[k]).T\n",
    "            weighted_sum = np.dot(self.resp[:, k] * diff, diff.T)\n",
    "            self.covs[k] = weighted_sum / (resp_weights[k]+self.eps)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        self._compute_log_likelihood(X_test)\n",
    "        return np.log(np.sum(self.resp, axis = 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2b8cd",
   "metadata": {
    "id": "33a2b8cd"
   },
   "outputs": [],
   "source": [
    "class GMM_classification():\n",
    "    \n",
    "    def __init__(self,n_classes,n_clusters):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_clusters = n_clusters\n",
    "    \n",
    "    def fit_predict(self,X_train,X_test,counts):\n",
    "        pred = np.zeros((X_test.shape[0],self.n_classes))\n",
    "        for i in range(len(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X_train[st_idx:st_idx+counts[i]]\n",
    "            clf = GMM(n_components = 3, n_iters = 50, tol = 1e-4).fit(curr_X)\n",
    "            pred[:,i] = clf.predict(X_test)\n",
    "        return np.argmax(pred,axis = 1)\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef5992",
   "metadata": {
    "id": "edef5992"
   },
   "outputs": [],
   "source": [
    "class GMM_generate():\n",
    "        \n",
    "    def __init__(self, n_features, n_clusters):\n",
    "        self.clusters = {}\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_features = n_features\n",
    "        self.eps = 1e-10\n",
    "\n",
    "        pi = np.random.rand(n_clusters)\n",
    "        self.clusters['pi'] = np.ones(n_clusters) / n_clusters\n",
    "        \n",
    "        print('self.clusters[pi]', self.clusters['pi'])\n",
    "        self.clusters['logdet'] = np.zeros(n_clusters, dtype=np.float64)\n",
    "        self.clusters['mu'] = np.random.rand(self.n_clusters, self.n_features) * 2 - 1\n",
    "\n",
    "        inv_cov = []\n",
    "        for i in range(n_clusters):\n",
    "            inv_cov.append(np.eye(self.n_features))\n",
    "        self.clusters['inv_cov'] = np.array(inv_cov)\n",
    "        self.clusters['cov'] = np.array(inv_cov)\n",
    "\n",
    "        print('pi shape', self.clusters['pi'].shape)\n",
    "        print('mu shape', self.clusters['mu'].shape)\n",
    "        print('det shape', self.clusters['logdet'].shape)\n",
    "        print('inv_cov shape', self.clusters['inv_cov'].shape)\n",
    "\n",
    "\n",
    "    def check_power(self, power):\n",
    "        if np.max(power) >= 710:\n",
    "            print(f'High Exponent Error  min:{str(np.min(power))}  max:{str(np.max(power))}  new:{str(np.max(power) - 700)}')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def expectation_step(self, X):\n",
    "        print('E step')\n",
    "        # calculate responsibility for each data point, for each class\n",
    "        self.resp = np.zeros((self.N, self.n_clusters), dtype=np.float64)\n",
    "        self.exp = np.zeros((self.N, self.n_clusters), dtype=np.float64)\n",
    "        self.log_likelihood = 0\n",
    "\n",
    "        for k in range(self.n_clusters):\n",
    "            for n in range(len(X)):\n",
    "                diff = (X[n] - self.clusters['mu'][k])\n",
    "                self.exp[n][k] = np.log(self.clusters['pi'][k]) - (self.clusters['logdet'][k] + np.dot(np.dot(diff.T, self.clusters['inv_cov'][k]), diff)\n",
    ")/2\n",
    "\n",
    "        validate(exp_n_k = self.exp)\n",
    "        validate(clusters_pi = self.clusters['pi'])\n",
    "        validate(clusters_det = self.clusters['logdet'])\n",
    "\n",
    "        for n in range(self.N):\n",
    "            self.exp[n] -= (np.max(self.exp[n]) - 700)\n",
    "            self.resp[n] = np.exp(self.exp[n], dtype=np.float64)\n",
    "\n",
    "        validate(resp_n_k = self.resp)\n",
    "\n",
    "        for n in range(self.N):\n",
    "            resp_sum = np.sum(self.resp[n])\n",
    "            self.log_likelihood += np.log(resp_sum)\n",
    "            self.resp[n] = self.resp[n] / resp_sum\n",
    "\n",
    "\n",
    "\n",
    "    def maximization_step(self, X):\n",
    "        print('M step')\n",
    "\n",
    "        N_k = np.sum(self.resp, axis=0) # get class responsibility\n",
    "        print('N_k', N_k)\n",
    "        for k in range(self.n_clusters):\n",
    "            \n",
    "            # find pi for each cluster\n",
    "            self.clusters['pi'][k] = N_k[k] / self.N\n",
    "\n",
    "            # find new mean for each cluster\n",
    "            self.clusters['mu'][k] = np.matmul(self.resp[:, k], X) / N_k[k]\n",
    "\n",
    "            # find covariance matrix for each cluster\n",
    "            cov = np.zeros((self.n_features, self.n_features), dtype=np.float64)\n",
    "            for n in range(self.N):\n",
    "                v = X[n] - self.clusters['mu'][k]\n",
    "                cov += self.resp[n][k] * np.outer(v, v)\n",
    "            cov /= N_k[k]\n",
    "\n",
    "            self.clusters['cov'][k] = cov\n",
    "            dg = np.empty(self.n_features)\n",
    "            dg.fill(1e-6)\n",
    "            cov += np.diag(dg)\n",
    "            (sign, lgdet) = np.linalg.slogdet(cov)\n",
    "            self.clusters['logdet'][k] = sign * lgdet\n",
    "\n",
    "            if not self.clusters['logdet'][k]:\n",
    "                print(cov)\n",
    "                raise Exception('Determinant has zero ' + str(self.clusters['logdet'][k]))\n",
    "\n",
    "            try:\n",
    "                self.clusters['inv_cov'][k] = scipy.linalg.inv(cov)\n",
    "            except ValueError:\n",
    "                print()\n",
    "    \n",
    "\n",
    "    def sample(self, n_samples, cluster=None):\n",
    "        probs = np.cumsum(self.clusters['pi']) \n",
    "        def get_cluster(prob):\n",
    "            for i in range(self.n_clusters):\n",
    "                if prob <= probs[i]:\n",
    "                    return i\n",
    "        samples = []\n",
    "        unif = np.random.uniform(0,1,n_samples)\n",
    "        for n in range(n_samples):\n",
    "            k = get_cluster(unif[n])\n",
    "            samples.append(np.random.default_rng().multivariate_normal(self.clusters['mu'][k], self.clusters['cov'][k]))\n",
    "\n",
    "        return np.array(samples)\n",
    "\n",
    "    def fit(self, X, n_epochs):\n",
    "\n",
    "        self.N = X.shape[0]\n",
    "        for i in range(n_epochs):\n",
    "            self.expectation_step(X)\n",
    "            self.maximization_step(X)\n",
    "            print('Epoch: ', i + 1, 'log_likelihood: ', self.log_likelihood)\n",
    "            disp_resp(self.resp, self.n_clusters)\n",
    "\n",
    "        print('\\n\\nPI values', self.clusters['pi'])\n",
    "\n",
    "    from scipy import linalg\n",
    "\n",
    "    def calculate_fid(self, mu1, sigma1, mu2, sigma2):\n",
    "        \n",
    "        # calculate sum squared difference between means\n",
    "        ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "\n",
    "        # calculate sqrt of product between cov\n",
    "        covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "        # check and correct imaginary numbers from sqrt\n",
    "        if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "        # calculate score\n",
    "        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "        return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b90f06",
   "metadata": {
    "code_folding": [],
    "id": "19b90f06",
    "outputId": "5b0837b7-133d-4e78-b743-b2482db79da9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 624/624 [26:40<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics of my Gaussian MLE = (0.375, 0, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2000 [00:00<?, ?it/s]C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_11392\\533166870.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:15<00:00, 130.17it/s]\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics of Elastic Net Logistic Regression = (0.8253205128205128, 0.8757126567844925, 0.7722222222222223)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2000 [00:00<?, ?it/s]C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_11392\\533166870.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:18<00:00, 110.62it/s]\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics of L1 Logistic Regression = (0.8589743589743589, 0.895734597156398, 0.8222222222222222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2000 [00:00<?, ?it/s]C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_11392\\533166870.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:22<00:00, 87.49it/s]\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics of L1 Logistic Regression = (0.8269230769230769, 0.876993166287016, 0.7735042735042735)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2000 [00:00<?, ?it/s]C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_11392\\533166870.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:21<00:00, 91.14it/s]\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics of L2 Logistic Regression = (0.8525641025641025, 0.8925233644859814, 0.8102564102564103)\n",
      "metrics of LDA = (0.8365384615384616, 0.8819444444444443, 0.7897435897435897)\n",
      "metrics of Multiclass Gaussian Naive Bayes = (0.8365384615384616, 0.8725, 0.8170940170940171)\n",
      "metrics of K nearest neighbor = (0.8381410256410257, 0.8829663962920046, 0.7918803418803418)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▎                                           | 283/624 [29:00<34:57,  6.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m q1_experiments \u001b[38;5;241m=\u001b[39m Experiments()\n\u001b[1;32m---> 18\u001b[0m \u001b[43mq1_experiments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Q2DataLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Assignment 1/data/bloodmnist.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m x_train,y_train,x_test,y_test,x_val,y_val \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtrain_test_split()\n",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36mExperiments.binary_experiments\u001b[1;34m(self, X, y, x_test, y_test, counts, dataset)\u001b[0m\n\u001b[0;32m     68\u001b[0m clf \u001b[38;5;241m=\u001b[39m GaussianMAP(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     69\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X,counts)\n\u001b[1;32m---> 70\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m accuracy,F1,AUC \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_metrics(pred,y_test)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics of my Gaussian MLE = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy,F1,AUC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36mGaussianMAP.predict\u001b[1;34m(self, x_test)\u001b[0m\n\u001b[0;32m     32\u001b[0m best_posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes):\n\u001b[1;32m---> 34\u001b[0m     log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     log_posterior \u001b[38;5;241m=\u001b[39m log_likelihood \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prior[j]\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_posterior \u001b[38;5;241m<\u001b[39m log_posterior:\n",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36mGaussianMAP.log_likelihood\u001b[1;34m(self, X, mu, cov)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_likelihood\u001b[39m(\u001b[38;5;28mself\u001b[39m,X, mu, cov):\n\u001b[0;32m     24\u001b[0m     sign,log_det \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mslogdet(cov)\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mlog_det \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mdot((X\u001b[38;5;241m-\u001b[39mmu)\u001b[38;5;241m.\u001b[39mT,\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m)\u001b[49m),X\u001b[38;5;241m-\u001b[39mmu)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpinv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_name\\lib\\site-packages\\numpy\\linalg\\linalg.py:1990\u001b[0m, in \u001b[0;36mpinv\u001b[1;34m(a, rcond, hermitian)\u001b[0m\n\u001b[0;32m   1988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap(res)\n\u001b[0;32m   1989\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mconjugate()\n\u001b[1;32m-> 1990\u001b[0m u, s, vt \u001b[38;5;241m=\u001b[39m \u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhermitian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhermitian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;66;03m# discard small singular values\u001b[39;00m\n\u001b[0;32m   1993\u001b[0m cutoff \u001b[38;5;241m=\u001b[39m rcond[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, newaxis] \u001b[38;5;241m*\u001b[39m amax(s, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_name\\lib\\site-packages\\numpy\\linalg\\linalg.py:1648\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n_s\n\u001b[0;32m   1647\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1648\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1649\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1650\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    dataset = Q1DataLoader('../Assignment 1/data/pneumoniamnist.npz')\n",
    "    x_train,y_train,x_test,y_test,x_val,y_val = dataset.train_test_split()\n",
    "    #x_train = dataset.normalize_input(x_train)\n",
    "    unique,counts = np.unique(y_train,return_counts= True)\n",
    "    X = np.zeros(x_train.shape)\n",
    "    y = np.zeros(y_train.shape[0],dtype = int)\n",
    "    idx = 0\n",
    "    for label in range(2):\n",
    "        labels = np.where(y_train==label)[0]\n",
    "        for i in range(np.size(labels)):\n",
    "            X[idx] = x_train[labels[i]]\n",
    "            y[idx] = label\n",
    "            idx += 1\n",
    "            \n",
    "    q1_experiments = Experiments()\n",
    "    q1_experiments.binary_experiments(X,y,x_test,y_test,counts,dataset)\n",
    "    \n",
    "    dataset = Q2DataLoader('../Assignment 1/data/bloodmnist.npz')\n",
    "    x_train,y_train,x_test,y_test,x_val,y_val = dataset.train_test_split()\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    X = np.zeros(x_train.shape)\n",
    "    y = np.zeros(y_train.shape[0],dtype = int)\n",
    "    idx = 0\n",
    "    for label in range(8):\n",
    "        labels = np.where(y_train==label)[0]\n",
    "        for i in range(np.size(labels)):\n",
    "            X[idx] = x_train[labels[i]]\n",
    "            y[idx] = label\n",
    "            idx += 1\n",
    "    \n",
    "    q2_experiments = Experiments()\n",
    "    q2_experiments.multi_class_experiments(X,y,x_test,y_test,counts,dataset)\n",
    "    \n",
    "    \n",
    "    ann_path = '../Assignment 1/data/Road Sign Detection/annotations/'\n",
    "    image_path = '../Assignment 1/data/Road Sign Detection/images/'\n",
    "    dataset = Q3DataLoader(ann_path,image_path)\n",
    "    X_train, X_test, y_train, y_test = dataset.train_test_split()\n",
    "    q3_experiments = Experiments()\n",
    "    q3_experiments.Regression_experiments(X_train,y_train,x_test,y_test,dataset)\n",
    "    \n",
    "    \n",
    "    path = \"../Assignment 1/data/data/\"\n",
    "    AudioFeatures = Q4DataLoader()\n",
    "    x_train,y_train,x_test,y_test = AudioFeatures.preprocessing_audio(path)\n",
    "    unique,counts = np.unique(y_train,return_counts= True)\n",
    "    X = np.zeros(x_train.shape)\n",
    "    y = np.zeros(y_train.shape[0],dtype = int)\n",
    "    idx = 0\n",
    "    for label in range(2):\n",
    "        labels = np.where(y_train == label)[0]\n",
    "        for i in range(np.size(labels)):\n",
    "            X[idx] = x_train[labels[i]]\n",
    "            y[idx] = label\n",
    "            idx += 1\n",
    "    \n",
    "    q4_experiments = Experiments()\n",
    "    q4_experiments.binary_experiments(X,y,x_test,y_test,counts)\n",
    "    \n",
    "\n",
    "    data = Q5DataLoader('/content/drive/MyDrive/IISc/PRNN/tiny-imagenet-200.zip')\n",
    "    X = data.extract()\n",
    "    NO_CLUSTERS = 5\n",
    "    NO_EPOCHS = 10\n",
    "    gmm = GMM_generate(X.shape[1], NO_CLUSTERS)\n",
    "    gmm.fit(X, NO_EPOCHS)\n",
    "    samples = gmm.sample(1000)\n",
    "    mu1, sigma1 = X.mean(axis=0), np.cov(X, rowvar=False)\n",
    "    mu2, sigma2 = samples[0].mean(axis=0), np.cov(samples[0],  rowvar=False)\n",
    "    fid = gmm.calculate_fid(mu1, sigma1, mu2, sigma2)\n",
    "    print(f'fid score: {fid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be4c28",
   "metadata": {
    "code_folding": [],
    "id": "60be4c28",
    "outputId": "fc289ff3-82f0-4ce1-a77f-002e440293be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [0.25785896 0.74214104] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dataset = Q1DataLoader('../Assignment 1/data/pneumoniamnist.npz')\n",
    "x_train,y_train,x_test,y_test,x_val,y_val = dataset.train_test_split()\n",
    "unique,counts = np.unique(y_train,return_counts = True)\n",
    "print(unique,counts/sum(counts),type(counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b83b51",
   "metadata": {
    "id": "85b83b51"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "test_pdf0 = [np.mean(multivariate_normal.pdf(x0,x_test[i],np.identity(x0.shape[1]))) for i in range(x_test.shape[0])]\n",
    "test_pdf1 = [np.mean(multivariate_normal.pdf(x1,x_test[i],np.identity(x1.shape[1]))) for i in range(x_test.shape[0])]\n",
    "\n",
    "pred_Parzen = []\n",
    "correct_preds = 0\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    if test_pdf1[i]>test_pdf0[i]:\n",
    "        pred_Parzen.append(1)\n",
    "        if y_test[i] == 1:\n",
    "            correct_preds += 1\n",
    "    else:\n",
    "        pred_Parzen.append(0)\n",
    "        if y_test[i] == 0:\n",
    "            correct_preds += 0\n",
    "print(correct_preds,pred_Parzen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c31078",
   "metadata": {
    "id": "10c31078"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16745a8",
   "metadata": {
    "id": "b16745a8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e11be",
   "metadata": {
    "id": "3e3e11be"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac949467",
   "metadata": {
    "code_folding": [],
    "id": "ac949467"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3af91",
   "metadata": {
    "id": "ecd3af91"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d456f1",
   "metadata": {
    "id": "b1d456f1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e3c8a",
   "metadata": {
    "id": "791e3c8a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d9048",
   "metadata": {
    "id": "535d9048"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b96c29",
   "metadata": {
    "code_folding": [],
    "id": "d1b96c29"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfadaf",
   "metadata": {
    "id": "51bfadaf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "846da1a3",
   "metadata": {
    "id": "846da1a3"
   },
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment1_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_name",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
