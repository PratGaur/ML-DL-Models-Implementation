{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87619259",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/apoorvrocks/Machine-Learning-Assignment-1/blob/main/Assignment2_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4d684",
   "metadata": {
    "id": "bed4d684",
    "outputId": "0e35b651-8939-4af3-faeb-c7c674f3ae15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\venv_name\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Asus\\anaconda3\\envs\\venv_name\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c67796",
   "metadata": {
    "id": "75c67796"
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def forwardPass(self, x,train) :\n",
    "        pass\n",
    "    \n",
    "    def backwardPass(self, grad_input) :\n",
    "        pass\n",
    "\n",
    "class ParamLayer(Layer):\n",
    "\n",
    "    def update(self, learning_rate) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa403638",
   "metadata": {
    "id": "fa403638"
   },
   "outputs": [],
   "source": [
    "class LinearLayer(ParamLayer):\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim,regularization,reg_rate=0.01):\n",
    "        self.weights = np.random.normal(0.0, 0.1,size=input_dim * output_dim).reshape( input_dim,output_dim)\n",
    "        self.weights = self.weights.astype('float64')\n",
    "        self.biases = np.zeros((1, output_dim)).astype('float64')\n",
    "        self.x = np.zeros(0)\n",
    "        self.grad_biases = np.zeros(0)\n",
    "        self.grad_weights = np.zeros(0)\n",
    "        self.regularization = regularization\n",
    "        self.reg_rate = reg_rate\n",
    "        self.optimizer = Adam()\n",
    "        \n",
    "    def forwardPass(self,x,train = True):\n",
    "        self.x = x \n",
    "        return np.dot(x,self.weights) + self.biases  \n",
    "    \n",
    "    def backwardPass(self,grad_input):\n",
    "        self.grad_weights = np.dot(self.x.T,grad_input)\n",
    "        self.grad_biases = np.sum(grad_input, axis=0, keepdims=True)\n",
    "        return np.dot(grad_input,self.weights.T)\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \n",
    "        if self.regularization==None:\n",
    "            grad_weights = self.grad_weights\n",
    "        elif self.regularization == 'L1':\n",
    "            grad_weights = self.grad_weights + self.reg_rate*np.sign(self.weights)\n",
    "        elif self.regularization == 'L2':\n",
    "            grad_weights = self.grad_weights + self.reg_rate*self.weights  \n",
    "        self.weights,self.biases = self.optimizer.update(self.weights,grad_weights,self.biases,self.grad_biases)\n",
    "        return self.weights,self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566f1cd",
   "metadata": {
    "id": "0566f1cd"
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.next_x = np.zeros(0)\n",
    "\n",
    "    def forwardPass(self, x,train = True):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def backwardPass(self, grad_input):\n",
    "        y = self.forwardPass(grad_input)\n",
    "        return y*(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a0f70",
   "metadata": {
    "id": "716a0f70"
   },
   "outputs": [],
   "source": [
    "class SigmoidLayer(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.next_x = np.zeros(0)\n",
    "\n",
    "    def forwardPass(self, x,train = True) :\n",
    "        self.next_x = 1. / (1. + np.exp(-x))\n",
    "        return self.next_x\n",
    "\n",
    "    def backwardPass(self, grad_input):\n",
    "        return (self.next_x * (1 - self.next_x)) * grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2caeab1",
   "metadata": {
    "id": "f2caeab1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.next_x = np.zeros(0)\n",
    "\n",
    "    def forwardPass(self, x,train = True) :\n",
    "        self.next_x = np.where(x >= 0, x, 0)\n",
    "        return self.next_x\n",
    "\n",
    "    def backwardPass(self, grad_input):\n",
    "        return np.where(grad_input >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebd30a",
   "metadata": {
    "id": "97ebd30a"
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Layer):\n",
    "\n",
    "    def __init__(self, y):\n",
    "        self.eps = 1e-3\n",
    "        self.y = np.clip(y, self.eps, 1-self.eps)\n",
    "        self.p = np.zeros(0)\n",
    "\n",
    "    def forwardPass(self, x,train = True):\n",
    "        self.p = np.clip(x, self.eps, 1.0 - self.eps)\n",
    "        return -(self.y * np.log(self.p) + (1-self.y) * np.log(1 - self.p))\n",
    "\n",
    "    def backwardPass(self, grad_input):\n",
    "        error = (self.p - self.y) / (self.p - self.p ** 2)\n",
    "        return error * grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08ee05",
   "metadata": {
    "id": "6c08ee05"
   },
   "outputs": [],
   "source": [
    "class MSELayer(Layer):\n",
    "\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "        self.x = np.zeros(0)\n",
    "\n",
    "    def forward(self, x, train = True) :\n",
    "        self.x = x\n",
    "        return np.square(x - self.y)\n",
    "\n",
    "    def backward(self, grad_input):\n",
    "        return -2 * (self.y - self.x) * grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677ea6e",
   "metadata": {
    "id": "4677ea6e"
   },
   "outputs": [],
   "source": [
    "class BatchNormLayer(ParamLayer):\n",
    "\n",
    "    def __init__(self, dims) :\n",
    "        self.gamma = np.ones((1, dims), dtype= 'float64')\n",
    "        self.bias = np.zeros((1, dims), dtype= 'float64')\n",
    "        self.eps = 1e-3\n",
    "\n",
    "        # forward params\n",
    "        self.var_x = np.zeros(0)\n",
    "        self.stddev_x = np.zeros(0)\n",
    "        self.x_minus_mean = np.zeros(0)\n",
    "        self.standard_x = np.zeros(0)\n",
    "        self.num_examples = 0\n",
    "        self.mean_x = np.zeros(0)\n",
    "        self.running_avg_gamma = 0.9\n",
    "        self.running_mean = np.zeros(0)\n",
    "        self.running_var = np.zeros(0)\n",
    "        \n",
    "        # backward params\n",
    "        self.gamma_grad = np.zeros(0)\n",
    "        self.beta_grad = np.zeros(0)\n",
    "        self.optimizer = Adam()\n",
    "        \n",
    "    def forwardPass(self, x, train=True) :\n",
    "        self.num_examples = x.shape[0]\n",
    "        #print(f\"in forward pass {self.num_examples}\")\n",
    "        if train:\n",
    "            self.mean_x = np.mean(x, axis=0, keepdims=True)\n",
    "            self.var_x = np.var(x , axis=0, keepdims=True)\n",
    "            if np.array_equal(np.zeros(0),self.running_mean):\n",
    "                self.running_mean = self.mean_x\n",
    "                self.running_var = self.var_x\n",
    "            else:\n",
    "                self.running_mean = self.running_avg_gamma*self.running_mean + (1-self.running_avg_gamma)*self.mean_x\n",
    "                self.running_var  = self.running_avg_gamma*self.running_var + (1-self.running_avg_gamma)*self.var_x\n",
    "        else:\n",
    "            self.mean_x = self.running_mean.copy()\n",
    "            self.var_x = self.running_var.copy()\n",
    "\n",
    "        self.var_x += self.eps\n",
    "        self.stddev_x = np.sqrt(self.var_x)\n",
    "        self.xmu = x - self.mean_x\n",
    "        self.standard_x = self.xmu / self.stddev_x\n",
    "        return self.gamma * self.standard_x + self.bias\n",
    "\n",
    "    def backwardPass(self, grad_input) :\n",
    "        \n",
    "        standard_grad = grad_input * self.gamma\n",
    "\n",
    "        var_grad = np.sum(standard_grad * self.xmu * -0.5 * self.var_x ** (-3/2),\n",
    "                          axis=0, keepdims=True)\n",
    "        stddev_inv = 1 / self.stddev_x\n",
    "        aux_xmu = 2 * self.xmu / self.num_examples\n",
    "\n",
    "        mean_grad = (np.sum(standard_grad * -stddev_inv, axis=0,\n",
    "                            keepdims=True) +\n",
    "                            var_grad * np.sum(-aux_xmu, axis=0,\n",
    "                            keepdims=True))\n",
    "\n",
    "        self.gamma_grad = np.sum(grad_input * self.standard_x, axis=0,\n",
    "                                 keepdims=True)\n",
    "        self.bias_grad = np.sum(grad_input, axis=0, keepdims=True)\n",
    "        return standard_grad * stddev_inv + var_grad * aux_xmu + mean_grad / self.num_examples\n",
    "               \n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \n",
    "        self.gamma,self.bias = self.optimizer.update(self.gamma,self.gamma_grad,self.bias,self.bias_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb3d16",
   "metadata": {
    "id": "15eb3d16"
   },
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    \n",
    "    def __init__(self,keep_prob):\n",
    "        \n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "    def forwardPass(self,x,train):\n",
    "        if train:\n",
    "            self.mask = np.random.rand(x.shape[0],x.shape[1])\n",
    "            self.mask = self.mask < self.keep_prob\n",
    "            x = np.multiply(x,self.mask)\n",
    "            x = x/self.keep_prob  \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def backwardPass(self,grad_input):\n",
    "\n",
    "        return grad_input*self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a225850",
   "metadata": {
    "id": "8a225850"
   },
   "outputs": [],
   "source": [
    "class StochasticGradientDescent():\n",
    "    def __init__(self, learning_rate=0.001, momentum=0):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.momentum = momentum\n",
    "        self.w_updt = None\n",
    "        self.b_updt = None\n",
    "    def update(self, w, grad_wrt_w,b,grad_wrt_b):\n",
    "        if self.w_updt is None:\n",
    "            self.w_updt = np.zeros(np.shape(w))\n",
    "        if self.b_updt is None:\n",
    "            self.b_updt = np.zeros(np.shape(b))\n",
    "        self.w_updt = self.momentum * self.w_updt + self.learning_rate * grad_wrt_w\n",
    "        self.b_updt = self.momentum * self.b_updt + self.learning_rate * grad_wrt_b\n",
    "        return w - self.w_updt, b - self.b_updt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb03ef",
   "metadata": {
    "id": "a5bb03ef"
   },
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    \n",
    "    def __init__(self, learning_rate= 3e-4, b1=0.9, b2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = 1e-8\n",
    "        self.m_w = None\n",
    "        self.v_w = None\n",
    "        self.m_b = None\n",
    "        self.v_b = None\n",
    "        # Decay rates\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "\n",
    "    def update(self, w, grad_wrt_w,b,grad_wrt_b):\n",
    "        # If not initialized\n",
    "        if self.m_w is None:\n",
    "            self.m_w = np.zeros(np.shape(grad_wrt_w))\n",
    "            self.v_w = np.zeros(np.shape(grad_wrt_w))\n",
    "        if self.m_b is None:\n",
    "            self.m_b = np.zeros(np.shape(grad_wrt_b))\n",
    "            self.v_b = np.zeros(np.shape(grad_wrt_b))\n",
    "        \n",
    "        self.m_w = self.b1 * self.m_w + (1 - self.b1) * grad_wrt_w\n",
    "        self.v_w = self.b2 * self.v_w + (1 - self.b2) * np.power(grad_wrt_w, 2)\n",
    "\n",
    "        self.m_b = self.b1 * self.m_b + (1 - self.b1) * grad_wrt_b\n",
    "        self.v_b = self.b2 * self.v_b + (1 - self.b2) * np.power(grad_wrt_b, 2)\n",
    "        \n",
    "        m_w_hat = self.m_w / (1 - self.b1)\n",
    "        v_w_hat = self.v_w / (1 - self.b2)\n",
    "        self.w_updt = self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.eps)\n",
    "        \n",
    "        m_b_hat = self.m_b / (1 - self.b1)\n",
    "        v_b_hat = self.v_b / (1 - self.b2)\n",
    "\n",
    "        self.b_updt = self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.eps)\n",
    "        \n",
    "\n",
    "        return w - self.w_updt,b-self.b_updt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R3mSNyzkChMu",
   "metadata": {
    "id": "R3mSNyzkChMu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MYDxglbVcaAu",
   "metadata": {
    "id": "MYDxglbVcaAu"
   },
   "outputs": [],
   "source": [
    "class CNN_Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ebr4GIGz31d6",
   "metadata": {
    "id": "Ebr4GIGz31d6"
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "class Convolutional(CNN_Layer):\n",
    "    def __init__(self, input_shape, kernel_size):\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = input_depth\n",
    "        self.input_shape = input_shape\n",
    "        self.kernel_depth = input_depth\n",
    "        self.output_shape = (input_depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernels_shape = (input_depth, input_depth, kernel_size, kernel_size)\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        # print('input', input.shape)\n",
    "        # print('conv in', self.input_shape)\n",
    "        # print('conv out', self.output_shape)\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.kernel_depth):\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.kernel_depth):\n",
    "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
    "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
    "\n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KyQUufPty9bK",
   "metadata": {
    "id": "KyQUufPty9bK"
   },
   "outputs": [],
   "source": [
    "class Dense(CNN_Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01Vdu5Uf5q_j",
   "metadata": {
    "id": "01Vdu5Uf5q_j"
   },
   "outputs": [],
   "source": [
    "class Reshape(CNN_Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, self.output_shape)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heEykKuT7xj8",
   "metadata": {
    "id": "heEykKuT7xj8"
   },
   "outputs": [],
   "source": [
    "class Softmax(CNN_Layer):\n",
    "    def forward(self, input):\n",
    "        tmp = np.exp(input)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        n = np.size(self.output)\n",
    "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583UWpC_3rU",
   "metadata": {
    "id": "2583UWpC_3rU"
   },
   "outputs": [],
   "source": [
    "class Maxpool(CNN_Layer):\n",
    "    def __init__(self, input_shape, kernel_size):\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = input_depth\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print('max input', input.shape)\n",
    "        # print('max in', self.input_shape)\n",
    "        # print('max out', self.output_shape)\n",
    "        output_conv = np.zeros(self.output_shape)\n",
    "        self.maxargs = np.zeros((self.output_shape[0], self.output_shape[1], self.output_shape[2], 2))\n",
    "        for d in range(self.depth):\n",
    "            for i in range(self.input_shape[1] - self.kernel_size + 1):\n",
    "                for j in range(self.input_shape[1] - self.kernel_size + 1):\n",
    "                    mat = input[d, i:self.kernel_size + i, j:self.kernel_size+j]\n",
    "                    indices = np.where(mat == mat.max())\n",
    "                    try:\n",
    "                        output_conv[d, i, j] = mat[indices[0][0], indices[1][0]]\n",
    "                        self.maxargs[d, i, j] = [indices[0][0]+i, indices[1][0]+j]\n",
    "                    except IndexError:\n",
    "                        print(indices)\n",
    "                        print(mat)\n",
    "                        raise Exception(\"Nan in maxpool\")\n",
    "        return output_conv\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        back_layer = np.zeros(self.input_shape)\n",
    "        for d in range(self.depth):\n",
    "            for i in range(self.input_shape[1] - self.kernel_size + 1):\n",
    "                for j in range(self.input_shape[1] - self.kernel_size + 1):\n",
    "                    ind = self.maxargs[d, i, j]\n",
    "                    back_layer[d, int(ind[0]), int(ind[1])] = output_gradient[d, i, j]\n",
    "        return back_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1qfzAkP-ayb5",
   "metadata": {
    "id": "1qfzAkP-ayb5"
   },
   "outputs": [],
   "source": [
    "class Avgpool(CNN_Layer):\n",
    "    def __init__(self, input_shape, kernel_size):\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = input_depth\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_conv = np.zeros(self.output_shape)\n",
    "        for d in range(self.depth):\n",
    "            for i in range(self.input_shape[1] - self.kernel_size + 1):\n",
    "                for j in range(self.input_shape[1] - self.kernel_size + 1):\n",
    "                    output_conv[d, i, j] = np.mean(input[d, i:self.kernel_size + i, j:self.kernel_size+j])\n",
    "        return output_conv\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        count_layer = np.zeros(self.input_shape)\n",
    "        back_layer = np.zeros(self.input_shape)\n",
    "        for d in range(self.depth):\n",
    "            for i in range(self.input_shape[1] - self.kernel_size + 1):\n",
    "                for j in range(self.input_shape[1] - self.kernel_size + 1):\n",
    "                    count_layer[d, i:self.kernel_size + i, j:self.kernel_size+j] += 1\n",
    "                    back_layer[d, i:self.kernel_size + i, j:self.kernel_size+j] += output_gradient[d, i, j]\n",
    "        return back_layer / count_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84DtoHjaCoM-",
   "metadata": {
    "id": "84DtoHjaCoM-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CUrLhYb614Ki",
   "metadata": {
    "id": "CUrLhYb614Ki"
   },
   "outputs": [],
   "source": [
    "class CNN_Activation(CNN_Layer):\n",
    "    def __init__(self, activation, activation_prime, *args):\n",
    "        self.params = args\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input, self.params)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input, self.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XFW1DzmFF2OJ",
   "metadata": {
    "id": "XFW1DzmFF2OJ"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, args):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime( x, args):\n",
    "    s = sigmoid(x, args)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(x, args):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x, args):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def relu(x, args):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_prime(x, args):\n",
    "    return (x>0).astype(x.dtype)\n",
    "\n",
    "def lkrelu(x, args):\n",
    "    a = np.maximum(x, 0)\n",
    "    b = np.minimum(x * args[0], 0)\n",
    "    return a + b\n",
    "\n",
    "def lkrelu_prime(x, args):\n",
    "    a = (x>0).astype(x.dtype)\n",
    "    b = (x<0).astype(x.dtype) * args[0]\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F_I_UTX-4yvD",
   "metadata": {
    "id": "F_I_UTX-4yvD"
   },
   "outputs": [],
   "source": [
    "class Tanh(CNN_Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "\n",
    "\n",
    "class Sigmoid(CNN_Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(sigmoid, sigmoid_prime)\n",
    "\n",
    "\n",
    "class Relu(CNN_Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(relu, relu_prime)\n",
    "\n",
    "\n",
    "class LkRelu(CNN_Activation):\n",
    "    def __init__(self, lk_const):\n",
    "        super().__init__(lkrelu, lkrelu_prime, lk_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x9aW33TD5VrM",
   "metadata": {
    "id": "x9aW33TD5VrM"
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yhz_uNV2CuU9",
   "metadata": {
    "id": "yhz_uNV2CuU9"
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, layers):\n",
    "        pass\n",
    "\n",
    "    def predict(self, network, input):\n",
    "        output = input\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def train(self, network, loss, loss_prime, x_train, y_train, epochs = 1000, learning_rate = 0.01, verbose = True):\n",
    "        errors = []\n",
    "        for e in range(epochs):\n",
    "            error = 0\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                # forward\n",
    "                output = self.predict(network, x)\n",
    "                \n",
    "                # error\n",
    "                error += loss(y, output)\n",
    "\n",
    "                # backward\n",
    "                grad = loss_prime(y, output)\n",
    "                for layer in reversed(network):\n",
    "                    grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "            error /= len(x_train)\n",
    "            errors.append(error)\n",
    "            if verbose:\n",
    "                print(f\"{e + 1}/{epochs}, error={error}\")\n",
    "        return errors\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a9cf9",
   "metadata": {
    "id": "b41a9cf9"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.loss_layer = None\n",
    "    \n",
    "    def backward_propagation(self, output) :\n",
    "        grad_input = self.loss_layer.backwardPass(np.ones_like(output)) \n",
    "        for layer_index in range(len(self.layers)-1, -1, -1):\n",
    "            grad_input = self.layers[layer_index].backwardPass(grad_input)\n",
    "\n",
    "    def update_params(self, learning_rate) :\n",
    "        for layer_index in range(len(self.layers)):\n",
    "            layer = self.layers[layer_index]\n",
    "            if isinstance(layer, ParamLayer):\n",
    "                layer.update(learning_rate)\n",
    "\n",
    "    def evaluate_model(self, test_x, \n",
    "                       test_y):\n",
    "        self.loss_layer = CrossEntropyLoss(test_y)\n",
    "        #self.loss_layer = MSELayer(test_y)\n",
    "        y_pred = self.predict(test_x)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        y_labels = np.argmax(test_y, axis=1)\n",
    "        loss = self.loss_layer.forwardPass(y_pred)\n",
    "        accuracy = float(np.sum(y_labels == y_pred_labels) / test_x.shape[0])\n",
    "        return float(np.mean(loss)),y_pred_labels,accuracy\n",
    "\n",
    "    def fit(self, x_train, y_train,\n",
    "            learning_rate, steps,\n",
    "            batch_size, x_test,\n",
    "            y_test) :\n",
    "        \n",
    "        num_examples = x_train.shape[0]\n",
    "        num_batches = math.ceil(num_examples/batch_size)\n",
    "        n_epochs = steps//num_batches\n",
    "        training_metrics = np.zeros((n_epochs,2))  # loss, accuracy\n",
    "        validation_metrics = np.zeros((n_epochs,2))\n",
    "        random_index = np.linspace(0, num_examples-1, num_examples).astype(int)\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            \n",
    "            np.random.shuffle(random_index)\n",
    "            x_train = x_train[random_index]\n",
    "            y_train = y_train[random_index]\n",
    "            for index_batch in range(0, num_examples, batch_size):\n",
    "                mini_batch_x = x_train[index_batch: index_batch + batch_size]\n",
    "                mini_batch_y = y_train[index_batch: index_batch + batch_size]\n",
    "                self.loss_layer = CrossEntropyLoss(mini_batch_y)\n",
    "                #self.loss_layer = MSELayer(mini_batch_y)\n",
    "                y_pred = self.predict(mini_batch_x,train = True)\n",
    "                self.loss_layer.forwardPass(y_pred)\n",
    "                self.backward_propagation(y_pred)\n",
    "                self.update_params(learning_rate)\n",
    "\n",
    "            tr_loss,tr_pred,tr_accuracy = self.evaluate_model(x_train, y_train)\n",
    "            val_loss,val_pred,val_accuracy = self.evaluate_model(x_test,y_test)\n",
    "            if epoch%50==0 : \n",
    "                print(f\"Epoch = {epoch} tr_loss = {tr_loss},tr_accuracy = {tr_accuracy} ,val_loss = {val_loss},val_accuracy={val_loss}\")\n",
    "            training_metrics[epoch,0] = tr_loss\n",
    "            training_metrics[epoch,1] = tr_accuracy\n",
    "            validation_metrics[epoch,0] = val_loss\n",
    "            validation_metrics[epoch,1] = val_accuracy\n",
    "\n",
    "        return training_metrics,validation_metrics\n",
    "\n",
    "    def predict(self, x_test,train = False) :\n",
    "        for layer in self.layers:\n",
    "            x_test = layer.forwardPass(x_test,train)\n",
    "        return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce27fb",
   "metadata": {
    "id": "a6ce27fb"
   },
   "outputs": [],
   "source": [
    "class Q1DataLoader():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = np.load('../Assignment 1/data/pneumoniamnist.npz')\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        x_train = self.data['train_images'].reshape(self.data['train_images'].shape[0],-1).astype(\"float32\")\n",
    "        x_test = self.data['test_images'].reshape(self.data['test_images'].shape[0],-1).astype(\"float32\")\n",
    "        x_val = self.data['val_images'].reshape(self.data['val_images'].shape[0],-1).astype(\"float32\")\n",
    "        y_train = self.data['train_labels'].reshape((-1,))\n",
    "        y_test = self.data['test_labels'].reshape((-1,))\n",
    "        y_val = self.data['val_labels'].reshape((-1,))\n",
    "        \n",
    "        return x_train,y_train,x_test,y_test,x_val,y_val\n",
    "    \n",
    "    def one_hot(self,y):\n",
    "        \n",
    "        one_hot_y = np.zeros((y.size, y.max()+1)).astype(\"float32\")\n",
    "        one_hot_y[np.arange(y.size), y] = 1\n",
    "\n",
    "        return one_hot_y \n",
    "    \n",
    "    def get_metrics(self,pred,actual):\n",
    "        \n",
    "        n_correct_preds = 0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                if actual[i]==1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "            else:\n",
    "                if actual[i]==1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "\n",
    "        accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "        if tp+fn==0:\n",
    "            recall= 0 \n",
    "        else:\n",
    "            recall = tp/(tp+fn)\n",
    "        if tp+fp==0:\n",
    "            precision = 0\n",
    "        else:   \n",
    "            precision = tp/(tp+fp)\n",
    "        if recall==0 and precision==0:\n",
    "            F1 = 0 \n",
    "        else:\n",
    "            F1 = 2*recall*precision/(recall+precision)\n",
    "        if tn+fp==0:\n",
    "            specificity = 0\n",
    "        else:\n",
    "            specificity = tn/(tn+fp)\n",
    "        AUC = (recall + specificity)/2\n",
    "\n",
    "        return accuracy,F1,AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9f163",
   "metadata": {
    "id": "b2c9f163"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "\n",
    "class Q2DataLoader():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = np.load('../Assignment 1/data/bloodmnist.npz')\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        x_train = self.data['train_images'].reshape(self.data['train_images'].shape[0],-1).astype(\"float32\")\n",
    "        x_test = self.data['test_images'].reshape(self.data['test_images'].shape[0],-1).astype(\"float32\")\n",
    "        x_val = self.data['val_images'].reshape(self.data['val_images'].shape[0],-1).astype(\"float32\")\n",
    "        y_train = self.data['train_labels'].reshape((-1,))\n",
    "        y_test = self.data['test_labels'].reshape((-1,))\n",
    "        y_val = self.data['val_labels'].reshape((-1,))\n",
    "    \n",
    "        return x_train,y_train,x_test,y_test,x_val,y_val\n",
    "    \n",
    "    \n",
    "    def confusion_mat(self,actual, pred):\n",
    "        \n",
    "        classes = 8\n",
    "        mat = np.zeros((8, 8))\n",
    "        for i, j in zip(actual, pred):\n",
    "            mat[i][j] += 1\n",
    "        return mat\n",
    "\n",
    "    def get_metrics(self, y_prediction,y_true):\n",
    "        \n",
    "        y_true = y_true.reshape((-1,)).astype(np.int64)\n",
    "        F1 = f1_score(y_true, y_prediction, average='macro')\n",
    "        precision = precision_score(y_true, y_prediction, average='macro')\n",
    "        recall = recall_score(y_true, y_prediction, average='macro')\n",
    "        ACC = accuracy_score(y_true, y_prediction)\n",
    "        AUC = (precision+recall)/2\n",
    "        '''\n",
    "        y_true = y_true.reshape((-1,)).astype(np.int64)\n",
    "        y_prediction = y_prediction.astype(np.int64)\n",
    "        cnf_matrix = confusion_mat(y_true, y_prediction)\n",
    "        # print(cnf_matrix)\n",
    "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "        TP = np.diag(cnf_matrix)\n",
    "        TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "        FP = FP.astype(float)\n",
    "        FN = FN.astype(float)\n",
    "        TP = TP.astype(float)\n",
    "        TN = TN.astype(float)\n",
    "        div = TP+FN\n",
    "        TPR = [0 if d==0 else 1/d for d in div]\n",
    "        TPR = TPR * TP\n",
    "\n",
    "        div = TN+FP\n",
    "        TNR = [0 if d==0 else 1/d for d in div]\n",
    "        TNR = TNR * TN\n",
    "\n",
    "        div = TP+FP\n",
    "        PPV = [0 if d==0 else 1/d for d in div]\n",
    "        PPV = PPV * TP\n",
    "\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        AUC = (TNR + TPR)/2\n",
    "        div = TPR + PPV\n",
    "        F1 = [0 if d==0 else 1/d for d in div]\n",
    "        F1 *= 2*TPR*PPV\n",
    "        '''\n",
    "        return ACC, F1, AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a12d2",
   "metadata": {
    "id": "b79a12d2"
   },
   "outputs": [],
   "source": [
    "class Q3Dataloader():\n",
    "    \n",
    "    def __init__(self,ann_path,im_path):\n",
    "        \n",
    "        self.ann_path = ann_path\n",
    "        self.im_path = im_path\n",
    "    \n",
    "    def get_file_list(self,root, file_type):\n",
    "        return [os.path.join(directory_path, f) for directory_path, directory_name, \n",
    "            files in os.walk(root) for f in files if f.endswith(file_type)]\n",
    "\n",
    "    def get_train_df(self,ann_path, img_path):\n",
    "    \n",
    "        ann_path_list = self.get_file_list(self.ann_path, '.xml')\n",
    "        ann = np.zeros((len(ann_path_list),4))\n",
    "        for i in range(len(ann_path_list)):\n",
    "            a_path = ann_path_list[i]\n",
    "            root = ET.parse(a_path).getroot()\n",
    "            ann[i][0] = int(root.find(\"./object/bndbox/xmin\").text)\n",
    "            ann[i][1] = int(root.find(\"./object/bndbox/ymin\").text)\n",
    "            ann[i][2] = int(root.find(\"./object/bndbox/xmax\").text)\n",
    "            ann[i][3] = int(root.find(\"./object/bndbox/ymax\").text)\n",
    "        return ann\n",
    "\n",
    "    def get_image_data(self):\n",
    "    \n",
    "        image_list = get_file_list(self.im_path,'png')\n",
    "        image_data = [ cv2.imread(image_path) for image_path in image_list]\n",
    "\n",
    "        return image_data\n",
    "    \n",
    "    def resize_image_bounding_box(self):\n",
    "        \n",
    "        image_data = self.get_image_data()\n",
    "        targetSize = (100,100)\n",
    "        image_list = get_file_list(self.im_path,'png')\n",
    "        resized_image_list = []\n",
    "        for i in range(len(image_data)):\n",
    "            x_scale = 100/image_data[i].shape[0]\n",
    "            y_scale = 100/image_data[i].shape[1]\n",
    "            train_box[i][0] = int(np.round(train_box[i][0]*x_scale))\n",
    "            train_box[i][1] = int(np.round(train_box[i][1]*y_scale))\n",
    "            train_box[i][2] = int(np.round(train_box[i][2]*x_scale))\n",
    "            train_box[i][3] = int(np.round(train_box[i][3]*y_scale))\n",
    "            image = cv2.imread(image_list[i])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ## grayscale image\n",
    "            norm_image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) ## normalize\n",
    "            image = cv2.resize(norm_image,targetSize)  ## resize to 100*100\n",
    "            #print(image.shape)\n",
    "            image = np.array(image)\n",
    "            resized_image_list.append(image.ravel())\n",
    "\n",
    "        return train_box,np.array(resized_image_list)  \n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        train_box,resized_image_list = self.resize_image_bounding_box()\n",
    "        return train_test_split(resized_image_list, train_box, test_size=0.3, random_state=34)\n",
    "    \n",
    "    def getMetrics(self,y_pred,actual):\n",
    "        \n",
    "        xA = max(y_pred[0], actual[0])\n",
    "        yA = max(y_pred[1], actual[1])\n",
    "        xB = min(y_pred[2], actual[2])\n",
    "        yB = min(y_pred[3], actual[3])\n",
    "        # compute the area of intersection rectangle\n",
    "        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "        # compute the area of both the prediction and ground-truth\n",
    "        # rectangles\n",
    "        boxAArea = (y_pred[2] - y_pred[0] + 1) * (y_pred[3] - y_pred[1] + 1)\n",
    "        boxBArea = (actual[2] - actual[0] + 1) * (actual[3] - actual[1] + 1)\n",
    "        # compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        # areas - the interesection area\n",
    "        iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "        # return the intersection over union value\n",
    "\n",
    "        MSE = np.mean(0.5 * (actual - y_pred)**2)\n",
    "        MAE = np.mean(np.abs(actual - y_pred))\n",
    "        \n",
    "        return MSE,MoU/y_pred.shape[0],MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3978789",
   "metadata": {
    "id": "e3978789"
   },
   "outputs": [],
   "source": [
    "class Q4DataLoader():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.phoneme = {}\n",
    "        phoneme_list = [\"bcl\",\"dcl\",\"gcl\",\"pcl\",\"tck\",\"kcl\",\"dcl\",\"tcl\",\"b\",\"d\",\"g\",\"p\",\"t\",\"k\",\"dx\",\"q\",\"jh\",\"ch\",\"s\",\"sh\",\"z\",\"zh\",\"f\",\"th\",\"v\",\"dh\",\"m\",\"n\",\"ng\",\"em\",\"en\",\"eng\",\"nx\",\"l\",\"r\",\"w\",\"y\",\"hh\",\"hv\",\"el\",\"iy\",\"ih\",\"eh\",\"ey\",\"ae\",\"aa\",\"aw\",\"ay\",\"ah\",\"ao\",\"oy\",\"ow\",\"uh\",\"uw\",\"ux\",\"er\",\"ax\",\"ix\",\"axr\",\"ax-h\",\"pau\",\"epi\",\"h#\"]\n",
    "        phonemlist_length = 63\n",
    "        #create key value dictionary\n",
    "        for ph in phoneme_list: \n",
    "            if('a' in ph or 'e' in ph or 'i' in ph or 'o' in ph or 'u' in ph):\n",
    "                self.phoneme[ph] = 0                                                ##phoneme is vowel\n",
    "            else:\n",
    "                self.phoneme[ph] = 1\n",
    "    \n",
    "    def get_max_feature_len(self,x):\n",
    "        \n",
    "        max_len = 0\n",
    "        n=x.__len__()\n",
    "        for i in range(n):\n",
    "            max_len = max(max_len,x[i].__len__())\n",
    "        return max_len\n",
    "    \n",
    "    def add_padding(self,x,max_len):\n",
    "    \n",
    "        x_train = []\n",
    "        n = x.__len__()\n",
    "        for i in range(n):\n",
    "            m=x[i].__len__()\n",
    "            temp=np.zeros(max_len)\n",
    "            if(m>max_len):\n",
    "                temp=x[i][:max_len]\n",
    "            else:\n",
    "                temp[:m]=x[i]\n",
    "\n",
    "            x_train.append(temp)\n",
    "        return x_train\n",
    "\n",
    "    def get_x_and_y(self,file_path):\n",
    "    \n",
    "        x = []\n",
    "        y = []\n",
    "        count = 0\n",
    "        for folder in os.listdir(file_path):\n",
    "            \n",
    "            path = file_path + folder + \"/\"\n",
    "            temp_name = \"\"\n",
    "            for files in os.listdir(path):\n",
    "                name = files.split(\".\")[0]\n",
    "                if name != temp_name:\n",
    "                    temp_name = name\n",
    "                wav_file = path + name + \".WAV\"\n",
    "                phn_file = path + name + \".PHN\"\n",
    "\n",
    "                data, sampling_freq = librosa.load(wav_file,sr=None, mono=True,offset=0.0,duration=None)\n",
    "                data=data.tolist()\n",
    "\n",
    "                file_obj = open(phn_file, 'r')\n",
    "                phonem_data = file_obj.readlines()\n",
    "                n=np.shape(phonem_data)[0]\n",
    "\n",
    "                for i in range(n):\n",
    "                    \n",
    "                    lower,upper,ph=phonem_data[i].split(\" \")\n",
    "                    lower = int(lower)\n",
    "                    upper = int(upper)\n",
    "                    ph = ph.replace(\"\\n\", \"\")\n",
    "                    temp = data[lower:upper]\n",
    "                    temp = np.array(temp)\n",
    "                    count += 1\n",
    "                    mfccs = librosa.feature.mfcc(temp, sr=sampling_freq)\n",
    "                    mfccs = mfccs.flatten()\n",
    "                    temp = mfccs.tolist()\n",
    "                    x.append(temp)\n",
    "                    y.append(self.phoneme[ph])\n",
    "                    \n",
    "        return x,y\n",
    "    \n",
    "    def preprocessing_audio(self,path):\n",
    "  \n",
    "        path_test  = path + \"test/DR1/\"\n",
    "        path_train = path + \"train/DR1/\"\n",
    "        x_train,y_train = self.get_x_and_y(path_train)         \n",
    "        x_test,y_test = self.get_x_and_y(path_test)\n",
    "        max_len = self.get_max_feature_len(x_train)\n",
    "        max_len = max(max_len,self.get_max_feature_len(x_test))\n",
    "        x_train = self.add_padding(x_train,max_len)\n",
    "        x_test = self.add_padding(x_test,max_len)\n",
    "\n",
    "        return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "    \n",
    "    def get_metrics(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                if actual[i]==1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "            else:\n",
    "                if actual[i]==1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "        \n",
    "        return tp/actual.shape[0],fp/actual.shape[0],tn/actual.shape[0],fn/actual.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XWUO-x-UQbpw",
   "metadata": {
    "id": "XWUO-x-UQbpw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "\n",
    "class TrafficDataLoader:\n",
    "    \n",
    "    def __init__(self,ann_path,im_path):\n",
    "        \n",
    "        self.ann_path = ann_path\n",
    "        self.im_path = im_path\n",
    "    \n",
    "    def get_file_list(self,root, file_type):\n",
    "        return [os.path.join(directory_path, f) for directory_path, directory_name, files in os.walk(root) for f in files if f.endswith(file_type)]\n",
    "\n",
    "    def get_train_df(self,ann_path, img_path):\n",
    "    \n",
    "        ann_path_list = self.get_file_list(self.ann_path, '.xml')\n",
    "        ann_path_list.sort()\n",
    "        ann = np.zeros((len(ann_path_list),4))\n",
    "        for i in range(len(ann_path_list)):\n",
    "            a_path = ann_path_list[i]\n",
    "            root = ET.parse(a_path).getroot()\n",
    "            ann[i][0] = int(root.find(\"./object/bndbox/xmin\").text)\n",
    "            ann[i][1] = int(root.find(\"./object/bndbox/ymin\").text)\n",
    "            ann[i][2] = int(root.find(\"./object/bndbox/xmax\").text)\n",
    "            ann[i][3] = int(root.find(\"./object/bndbox/ymax\").text)\n",
    "        return ann\n",
    "\n",
    "    def get_image_data(self):\n",
    "    \n",
    "        image_list = self.get_file_list(self.im_path,'png')\n",
    "        image_list.sort()\n",
    "        image_data = [ cv2.imread(image_path) for image_path in image_list]\n",
    "        return image_data\n",
    "    \n",
    "    def resize_image_bounding_box(self):\n",
    "        \n",
    "        image_data = self.get_image_data()\n",
    "        targetSize = (100,100)\n",
    "        image_list = self.get_file_list(self.im_path,'png')\n",
    "        resized_image_list = []\n",
    "        train_box = self.get_train_df(self.ann_path, self.im_path)\n",
    "        for i in range(len(image_data)):\n",
    "            x_scale = 100/image_data[i].shape[1]\n",
    "            y_scale = 100/image_data[i].shape[0]\n",
    "            train_box[i][0] = int(np.round(train_box[i][0]*x_scale))\n",
    "            train_box[i][1] = int(np.round(train_box[i][1]*y_scale))\n",
    "            train_box[i][2] = int(np.round(train_box[i][2]*x_scale))\n",
    "            train_box[i][3] = int(np.round(train_box[i][3]*y_scale))\n",
    "\n",
    "            image = cv2.resize(image_data[i],targetSize)  ## resize to 100*100\n",
    "            image = np.mean(image, axis=2)\n",
    "            image = np.array(image)\n",
    "            resized_image_list.append(image)\n",
    "\n",
    "        return train_box,np.array(resized_image_list)  \n",
    "    \n",
    "    def split(self):\n",
    "        \n",
    "        train_box, resized_image_list = self.resize_image_bounding_box()\n",
    "        print(train_box.shape, resized_image_list.shape)\n",
    "        return train_test_split(resized_image_list, train_box, test_size=0.3, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65029cef",
   "metadata": {
    "id": "65029cef"
   },
   "outputs": [],
   "source": [
    "class MLP_Experiments():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def one_hot(self,y):\n",
    "        \n",
    "        one_hot_y = np.zeros((y.size, y.max()+1)).astype(\"float32\")\n",
    "        one_hot_y[np.arange(y.size), y] = 1\n",
    "\n",
    "        return one_hot_y \n",
    "\n",
    "    def experiments(self,dataset):\n",
    "\n",
    "        x_train,y_train,x_test,y_test,x_val,y_val = dataset.train_test_split()\n",
    "        y_train_hot = self.one_hot(y_train)\n",
    "        y_test_hot = self.one_hot(y_test)\n",
    "        y_val_hot = self.one_hot(y_val)\n",
    "    \n",
    "        learning_rate = 1e-3\n",
    "        batch_size = 100\n",
    "        neurons = 100\n",
    "        steps = 10000\n",
    "        model_baseline = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,None),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, y_train_hot.shape[1],None),\n",
    "                                SigmoidLayer()])\n",
    "\n",
    "        baseline_tr_metrics,baseline_val_metrics = model_baseline.fit(x_train, y_train_hot, steps=steps,\n",
    "                                               learning_rate=learning_rate,\n",
    "                                               batch_size=batch_size,\n",
    "                                               x_test = x_test,y_test = y_test_hot)\n",
    "        baseline_tr_loss = baseline_tr_metrics[:,0]\n",
    "        baseline_tr_accuracy = baseline_tr_metrics[:,1]\n",
    "        baseline_val_loss = baseline_val_metrics[:,0]\n",
    "        baseline_val_accuracy = baseline_val_metrics[:,1]\n",
    "        baseline_pred = model_baseline.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "        print(f'baseline metrics on test data= {dataset.get_metrics(baseline_pred,y_test)}')\n",
    "\n",
    "        model_batchNorm = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,None),\n",
    "                                    BatchNormLayer(neurons),\n",
    "                                    SigmoidLayer(),\n",
    "                                    LinearLayer(neurons, y_train_hot.shape[1],None),\n",
    "                                    SigmoidLayer()])\n",
    "        batchNorm_tr_metrics,batchNorm_val_metrics = model_batchNorm.fit(x_train, y_train_hot, steps=steps,\n",
    "                                               learning_rate=learning_rate,\n",
    "                                               batch_size=batch_size,\n",
    "                                               x_test = x_test,y_test = y_test_hot)\n",
    "        batchNorm_tr_loss = batchNorm_tr_metrics[:,0]\n",
    "        batchNorm_tr_accuracy = batchNorm_tr_metrics[:,1]\n",
    "        batchNorm_val_loss = batchNorm_val_metrics[:,0]\n",
    "        batchNorm_val_accuracy = batchNorm_val_metrics[:,1]\n",
    "        batchNorm_pred = model_batchNorm.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "\n",
    "        print(f'batchNorm metrics = {dataset.get_metrics(batchNorm_pred,y_test)}')\n",
    "\n",
    "        model_dropout = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,None),\n",
    "                                    SigmoidLayer(),\n",
    "                                    Dropout(0.8),\n",
    "                                    LinearLayer(neurons, y_train_hot.shape[1],None),\n",
    "                                    SigmoidLayer()])\n",
    "        dropout_tr_metrics, dropout_val_metrics= model_dropout.fit(x_train, y_train_hot, steps=steps,\n",
    "                                               learning_rate=learning_rate,\n",
    "                                               batch_size=batch_size,\n",
    "                                               x_test = x_test,y_test = y_test_hot)\n",
    "        dropout_tr_loss = dropout_tr_metrics[:,0]\n",
    "        dropout_tr_accuracy = dropout_tr_metrics[:,1]\n",
    "        dropout_val_loss = dropout_val_metrics[:,0]\n",
    "        dropout_val_accuracy = dropout_val_metrics[:,1]\n",
    "        dropout_pred = model_dropout.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "\n",
    "        print(f'dropout metrics = {dataset.get_metrics(dropout_pred,y_test)}')\n",
    "\n",
    "        model_L1 = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,None),\n",
    "                                    SigmoidLayer(),\n",
    "                                    LinearLayer(neurons, y_train_hot.shape[1],None),\n",
    "                                    SigmoidLayer()])\n",
    "        L1_tr_metrics,L1_val_metrics = model_L1.fit(x_train, y_train_hot, steps=steps,\n",
    "                                               learning_rate=learning_rate,\n",
    "                                               batch_size=batch_size,\n",
    "                                               x_test = x_test,y_test = y_test_hot)\n",
    "        L1_tr_loss = L1_tr_metrics[:,0]\n",
    "        L1_tr_accuracy = L1_tr_metrics[:,1]\n",
    "        L1_val_loss = L1_val_metrics[:,0]\n",
    "        L1_val_accuracy = L1_val_metrics[:,1]\n",
    "        L1_pred = model_L1.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "        print(f'L1 metrics = {dataset.get_metrics(L1_pred,y_test)}')\n",
    "\n",
    "        model_L2 = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,None),\n",
    "                                    SigmoidLayer(),\n",
    "                                    LinearLayer(neurons, y_train_hot.shape[1],None),\n",
    "                                    SigmoidLayer()])\n",
    "\n",
    "\n",
    "        L2_tr_metrics,L2_val_metrics = model_L2.fit(x_train, y_train_hot, steps=steps,\n",
    "                                               learning_rate=learning_rate,\n",
    "                                               batch_size=batch_size,\n",
    "                                               x_test = x_test,y_test = y_test_hot)\n",
    "        L2_tr_loss = L2_tr_metrics[:,0]\n",
    "        L2_tr_accuracy = L2_tr_metrics[:,1]\n",
    "        L2_val_loss = L2_val_metrics[:,0]\n",
    "        L2_val_accuracy = L2_val_metrics[:,1]\n",
    "        L2_pred = model_L2.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "        print(f'L2 metrics = {dataset.get_metrics(L2_pred,y_test)}')\n",
    "\n",
    "        num_batches = math.ceil(x_train.shape[0] / batch_size)\n",
    "        steps_axis = np.linspace(1, steps/num_batches, len(baseline_val_loss)//10+1)\n",
    "        steps_axis = steps_axis.astype(\"int\")\n",
    "\n",
    "        plt.plot(baseline_val_loss, label=\"Baseline\")\n",
    "        plt.plot(batchNorm_val_loss, label=\"BatchNorm\")\n",
    "        plt.plot(dropout_val_loss, label=\"Dropout\")\n",
    "        plt.plot(L1_val_loss,label = \"L1\")\n",
    "        plt.plot(L2_val_loss,label = \"L2\")\n",
    "        plt.ylabel(\"Validation Loss\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.title(\"Loss versus Iterations\")\n",
    "        locs = np.linspace(0, len(baseline_val_loss)-1, len(baseline_val_loss)//10+1)\n",
    "        #plt.xticks(locs, [str(x)+\"k\" for x in steps_axis])\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(baseline_tr_loss, label=\"Baseline\")\n",
    "        plt.plot(batchNorm_tr_loss, label=\"BatchNorm\")\n",
    "        plt.plot(dropout_tr_loss, label=\"Dropout\")\n",
    "        plt.plot(L1_tr_loss,label = \"L1\")\n",
    "        plt.plot(L2_tr_loss,label = \"L2\")\n",
    "        plt.ylabel(\"Training Loss\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.title(\"Loss versus Iterations\")\n",
    "        locs = np.linspace(0, len(baseline_tr_loss)-1, len(baseline_tr_loss)//10+1)\n",
    "        #plt.xticks(locs, [str(x)+\"k\" for x in steps_axis])\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(baseline_val_accuracy, label=\"Baseline\")\n",
    "        plt.plot(batchNorm_val_accuracy, label=\"BatchNorm\")\n",
    "        plt.plot(dropout_val_accuracy, label=\"Dropout\")\n",
    "        plt.plot(L1_val_accuracy,label = \"L1\")\n",
    "        plt.plot(L2_val_accuracy,label = \"L2\")\n",
    "        plt.ylabel(\"Validation Accuracy\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.title(\"Accuracy versus iterations\")\n",
    "        locs = np.linspace(0, len(baseline_val_loss)-1, len(baseline_val_loss)//10+1)\n",
    "        #plt.xticks(locs, [str(x)+\"k\" for x in steps_axis])\n",
    "        plt.grid() \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(baseline_tr_accuracy, label=\"Baseline\")\n",
    "        plt.plot(batchNorm_tr_accuracy, label=\"BatchNorm\")\n",
    "        plt.plot(dropout_tr_accuracy, label=\"Dropout\")\n",
    "        plt.plot(L1_tr_accuracy,label = \"L1\")\n",
    "        plt.plot(L2_tr_accuracy,label = \"L2\")\n",
    "        plt.ylabel(\"Training Accuracy\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.title(\"Accuracy versus Iterations\")\n",
    "        locs = np.linspace(0, len(baseline_tr_accuracy)-1, len(baseline_tr_accuracy)//10+1)\n",
    "        #plt.xticks(locs, [str(x)+\"k\" for x in steps_axis])\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        ## 2 layer neural Networks \n",
    "\n",
    "\n",
    "        model_baseline =NeuralNetwork([LinearLayer(x_train.shape[1], neurons,None),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, neurons,None),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, y_train_hot.shape[1],None),\n",
    "                                SigmoidLayer()])\n",
    "        baseline_tr_metrics,baseline_val_metrics = model_baseline.fit(x_train, y_train_hot, steps=steps,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           x_test = x_test,y_test = y_test_hot)\n",
    "        baseline_tr_loss = baseline_tr_metrics[:,0]\n",
    "        baseline_tr_accuracy = baseline_tr_metrics[:,1]\n",
    "        baseline_val_loss = baseline_val_metrics[:,0]\n",
    "        baseline_val_accuracy = baseline_val_metrics[:,1]\n",
    "\n",
    "        baseline_pred = model_baseline.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "        print(f'baseline metrics on test data = {dataset.get_metrics(baseline_pred,y_test)}')\n",
    "\n",
    "        model_batchNorm = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,None),\n",
    "                                BatchNormLayer(neurons),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, neurons,None),\n",
    "                                BatchNormLayer(neurons),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, y_train_hot.shape[1],None),\n",
    "                                SigmoidLayer()])\n",
    "        batchNorm_tr_metrics,batchNorm_val_metrics = model_batchNorm.fit(x_train, y_train_hot, steps=steps,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           x_test = x_test,y_test = y_test_hot)\n",
    "        batchNorm_tr_loss = batchNorm_tr_metrics[:,0]\n",
    "        batchNorm_tr_accuracy = batchNorm_tr_metrics[:,1]\n",
    "        batchNorm_val_loss = batchNorm_val_metrics[:,0]\n",
    "        batchNorm_val_accuracy = batchNorm_val_metrics[:,1]\n",
    "        batchNorm_pred = model_batchNorm.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "\n",
    "        print(f'batchNorm metrics = {dataset.get_metrics(batchNorm_pred,y_test)}')\n",
    "\n",
    "        model_dropout = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,None),\n",
    "                                SigmoidLayer(),\n",
    "                                Dropout(0.8),\n",
    "                                LinearLayer(neurons, neurons,None),\n",
    "                                SigmoidLayer(),\n",
    "                                Dropout(0.6),\n",
    "                                LinearLayer(neurons, y_train_hot.shape[1],None),\n",
    "                                SigmoidLayer()])\n",
    "        dropout_tr_metrics, dropout_val_metrics= model_dropout.fit(x_train, y_train_hot, steps=steps,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           x_test = x_test,y_test = y_test_hot)\n",
    "        dropout_tr_loss = dropout_tr_metrics[:,0]\n",
    "        dropout_tr_accuracy = dropout_tr_metrics[:,1]\n",
    "        dropout_val_loss = dropout_val_metrics[:,0]\n",
    "        dropout_val_accuracy = dropout_val_metrics[:,1]\n",
    "        dropout_pred = model_dropout.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "\n",
    "        print(f'dropout metrics = {dataset.get_metrics(dropout_pred,y_test)}')\n",
    "\n",
    "        model_L1 = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,'L1'),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, neurons,'L1'),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, y_train_hot.shape[1],'L1'),\n",
    "                                SigmoidLayer()])\n",
    "        L1_tr_metrics,L1_val_metrics = model_L1.fit(x_train, y_train_hot, steps=steps,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           x_test = x_test,y_test = y_test_hot)\n",
    "        L1_tr_loss = L1_tr_metrics[:,0]\n",
    "        L1_tr_accuracy = L1_tr_metrics[:,1]\n",
    "        L1_val_loss = L1_val_metrics[:,0]\n",
    "        L1_val_accuracy = L1_val_metrics[:,1]\n",
    "        L1_pred = model_L1.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "\n",
    "        print(f'L1 metrics = {dataset.get_metrics(L1_pred,y_test)}')\n",
    "\n",
    "        model_L2 = NeuralNetwork([LinearLayer(x_train.shape[1], neurons,'L2'),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, neurons,'L2'),\n",
    "                                SigmoidLayer(),\n",
    "                                LinearLayer(neurons, y_train_hot.shape[1],'L2'),\n",
    "                                SigmoidLayer()])\n",
    "\n",
    "\n",
    "        L2_tr_metrics,L2_val_metrics = model_L2.fit(x_train, y_train_hot, steps=steps,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           x_test = x_test,y_test = y_test_hot)\n",
    "        L2_tr_loss = L2_tr_metrics[:,0]\n",
    "        L2_tr_accuracy = L2_tr_metrics[:,1]\n",
    "        L2_val_loss = L2_val_metrics[:,0]\n",
    "        L2_val_accuracy = L2_val_metrics[:,1]\n",
    "        L2_pred = model_L2.evaluate_model(x_test,y_test_hot)[1]\n",
    "\n",
    "        print(f'L2 metrics = {dataset.get_metrics(L2_pred,y_test)}')\n",
    "\n",
    "        num_batches = math.ceil(x_train.shape[0] / batch_size)\n",
    "        steps_axis = np.linspace(1, steps/num_batches, len(baseline_val_loss)//10+1)\n",
    "        steps_axis = steps_axis.astype(\"int\")\n",
    "\n",
    "        plt.plot(baseline_val_loss, label=\"Baseline\")\n",
    "        plt.plot(batchNorm_val_loss, label=\"BatchNorm\")\n",
    "        plt.plot(dropout_val_loss, label=\"Dropout\")\n",
    "        plt.plot(L1_val_loss,label = \"L1\")\n",
    "        plt.plot(L2_val_loss,label = \"L2\")\n",
    "        plt.ylabel(\"Validation Loss\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.title(\"Loss versus Iterations\")\n",
    "        locs = np.linspace(0, len(baseline_val_loss)-1, len(baseline_val_loss)//10+1)\n",
    "        #plt.xticks(locs, [str(x)+\"k\" for x in steps_axis])\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(baseline_tr_loss, label=\"Baseline\")\n",
    "        plt.plot(batchNorm_tr_loss, label=\"BatchNorm\")\n",
    "        plt.plot(dropout_tr_loss, label=\"Dropout\")\n",
    "        plt.plot(L1_tr_loss,label = \"L1\")\n",
    "        plt.plot(L2_tr_loss,label = \"L2\")\n",
    "        plt.ylabel(\"Training Loss\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.title(\"Loss versus Iterations\")\n",
    "        locs = np.linspace(0, len(baseline_tr_loss)-1, len(baseline_tr_loss)//10+1)\n",
    "        #plt.xticks(locs, [str(x)+\"k\" for x in steps_axis])\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(baseline_val_accuracy, label=\"Baseline\")\n",
    "        plt.plot(batchNorm_val_accuracy, label=\"BatchNorm\")\n",
    "        plt.plot(dropout_val_accuracy, label=\"Dropout\")\n",
    "        plt.plot(L1_val_accuracy,label = \"L1\")\n",
    "        plt.plot(L2_val_accuracy,label = \"L2\")\n",
    "        plt.ylabel(\"Validation Accuracy\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.title(\"Accuracy versus iterations\")\n",
    "        locs = np.linspace(0, len(baseline_val_loss)-1, len(baseline_val_loss)//10+1)\n",
    "        #plt.xticks(locs, [str(x)+\"k\" for x in steps_axis])\n",
    "        plt.grid() \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(baseline_tr_accuracy, label=\"Baseline\")\n",
    "        plt.plot(batchNorm_tr_accuracy, label=\"BatchNorm\")\n",
    "        plt.plot(dropout_tr_accuracy, label=\"Dropout\")\n",
    "        plt.plot(L1_tr_accuracy,label = \"L1\")\n",
    "        plt.plot(L2_tr_accuracy,label = \"L2\")\n",
    "        plt.ylabel(\"Training Accuracy\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.title(\"Accuracy versus Iterations\")\n",
    "        locs = np.linspace(0, len(baseline_tr_accuracy)-1, len(baseline_tr_accuracy)//10+1)\n",
    "        #plt.xticks(locs, [str(x)+\"k\" for x in steps_axis])\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114183c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, f1_score\n",
    "\n",
    "class SVM_Experiments():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def run_SVM(self,X_train,x_test,y_train,y_test,C,kernel):\n",
    "        classifier = SVC(kernel = kernel, C, random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        return acc,f1,(precision+recall)/2\n",
    "        \n",
    "    def experiments(self,dataset):\n",
    "        x_train,y_train,x_test,y_test,x_val,y_val = dataset.train_test_split()\n",
    "        C = [0.01,1,5]\n",
    "        kernels = ['linear','poly','rbf']\n",
    "        for c in C:\n",
    "            for kernel in kernels:\n",
    "                acc,f1,auc = self.run_SVM(x_train,x_test,y_train,y_test,c,kernel)\n",
    "                print(f\"Accuracy ={acc} F1  = {f1} auc ={auc} for C= {c} kernel = {kernel}\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b20689",
   "metadata": {
    "id": "08b20689"
   },
   "outputs": [],
   "source": [
    "class MnistResNet(nn.Module):\n",
    "    def __init__(self,in_channels,n_classes):\n",
    "        super(MnistResNet, self).__init__()\n",
    "\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "\n",
    "        self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Change the output layer to output 10 classes instead of 1000 classes\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class MnistVGG19(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_channels,n_classes):\n",
    "        super(MnistVGG19, self).__init__()\n",
    "        self.model = models.vgg19(pretrained=True)\n",
    "        in_features = self.model.classifier[-1].in_features\n",
    "        self.model.classifier[-1] = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ae85e",
   "metadata": {
    "id": "7b1ae85e"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = Image.fromarray(self.data[index].astype(np.uint8))\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00eda7",
   "metadata": {
    "id": "9f00eda7"
   },
   "outputs": [],
   "source": [
    "class Standard_CNN_Models():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def dataLoading(self,dataset):\n",
    "        \n",
    "        train_x,train_y,val_x,val_y,test_x,test_y = dataset.train_test_split()\n",
    "        transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5 ), (0.5,0.5,0.5))])\n",
    "        train_dataset = MyDataset(train_x, train_y, transform = transform)\n",
    "        val_dataset = MyDataset(val_x, val_y, transform = transform) \n",
    "        test_dataset = MyDataset(test_x, test_y, transform = None) \n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "        self.test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "        self.val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "        \n",
    "    def train_plots(self,model):\n",
    "        \n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) \n",
    "\n",
    "        start_ts = time.time()\n",
    "\n",
    "        losses = []\n",
    "        batches = len(self.train_dataloader)\n",
    "        val_batches = len(self.val_dataloader)\n",
    "        AUC, F1, accuracy = [],[],[]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress = tqdm(enumerate(self.train_dataloader), desc=\"Loss: \", total=batches)\n",
    "            model.train()\n",
    "\n",
    "            for i, data in progress:\n",
    "                X, y = data[0].to(device), data[1].to(device)\n",
    "                model.zero_grad()\n",
    "                outputs = model(X)\n",
    "                loss = loss_function(outputs, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                current_loss = loss.item()\n",
    "                total_loss += current_loss\n",
    "                progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            val_losses = 0\n",
    "            AUC_l, f1_l, accuracy_l = [], [], []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(self.val_dataloader):\n",
    "                    X, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                    outputs = model(X)\n",
    "\n",
    "                    val_losses += loss_function(outputs, y)\n",
    "\n",
    "                    predicted_classes = torch.max(outputs, 1)[1] \n",
    "\n",
    "                    a,f1,auc = Q1DataLoader().get_metrics(y.cpu(),predicted_classes.cpu())\n",
    "                    AUC_l.append(auc)\n",
    "                    f1_l.append(f1)\n",
    "                    accuracy_l.append(a)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
    "            print(f\"AUC = {sum(AUC_l)/len(AUC_l)},f1 = {sum(f1_l)/len(f1_l)},accuracy = {sum(accuracy_l)/len(accuracy_l)}\")\n",
    "            AUC.append(sum(AUC_l)/len(AUC_l))\n",
    "            accuracy.append(sum(accuracy_l)/len(accuracy_l))\n",
    "            F1.append(sum(f1_l)/len(f1_l))\n",
    "            losses.append(total_loss/batches) # for plotting learning curve\n",
    "\n",
    "            print(f\"Training time: {time.time()-start_ts}s\")\n",
    "            plt.plot(np.arange(0, len(losses)),losses)\n",
    "            plt.ylabel(\"Training Loss\")\n",
    "            plt.xlabel(\"Steps\")\n",
    "            plt.title(\"Loss versus Iterations\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(np.arange(0, len(losses)),accuracy,label = \"accuracy\")\n",
    "            plt.plot(np.arange(0, len(losses)),F1,label = \"F1\")\n",
    "            plt.plot(np.arange(0, len(losses)),AUC,label = \"AUC\")\n",
    "            plt.ylabel(\"Metrics\")\n",
    "            plt.xlabel(\"Steps\")\n",
    "            plt.title(\"Metrics versus Iterations\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "    def experiments(self,dataset,in_channels,n_classes):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\") \n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        self.dataLoading(dataset)\n",
    "        \n",
    "        model = MnistResNet(in_channels,n_classes).to(device)\n",
    "        self.train_plots(model)\n",
    "        model = MnistVGG19(in_channels,n_classes).to(device)\n",
    "        self.train_plots(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4jH-8I89E_yR",
   "metadata": {
    "id": "4jH-8I89E_yR"
   },
   "outputs": [],
   "source": [
    "class CNN_Experiments():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_error(self, lkrelu, tan, sig, title_name):\n",
    "        x = range(1, 21)\n",
    "        \n",
    "        plt.plot(x, lkrelu, label = \"lkrelu\")\n",
    "        plt.plot(x, tan, label = \"tan\")\n",
    "        plt.plot(x, sig, label = \"sig\")\n",
    "\n",
    "        plt.xlabel('iteration')\n",
    "\n",
    "        plt.ylabel('training error')\n",
    "        plt.xticks(np.arange(0, 21, 5.0))\n",
    "        plt.legend()\n",
    "\n",
    "        plt.title(title_name)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def get_metrics(self, y_true, y_prediction):\n",
    "        cnf_matrix = confusion_mat(y_true, y_prediction)\n",
    "        # print(cnf_matrix)\n",
    "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "        TP = np.diag(cnf_matrix)\n",
    "        TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "        FP = FP.astype(float)\n",
    "        FN = FN.astype(float)\n",
    "        TP = TP.astype(float)\n",
    "        TN = TN.astype(float)\n",
    "        div = TP+FN\n",
    "        TPR = [0 if d==0 else 1/d for d in div]\n",
    "        TPR = TPR * TP\n",
    "\n",
    "        div = TN+FP\n",
    "        TNR = [0 if d==0 else 1/d for d in div]\n",
    "        TNR = TNR * TN\n",
    "\n",
    "        div = TP+FP\n",
    "        PPV = [0 if d==0 else 1/d for d in div]\n",
    "        PPV = PPV * TP\n",
    "\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        AUC = (TNR + TPR)/2\n",
    "        div = TPR + PPV\n",
    "        F1 = [0 if d==0 else 1/d for d in div]\n",
    "        F1 *= 2*TPR*PPV\n",
    "\n",
    "        return ACC, F1, AUC\n",
    "\n",
    "    def confusion_mat(self, actual, pred):\n",
    "        classes = 8\n",
    "        mat = np.zeros((8, 8))\n",
    "        \n",
    "        for i, j in zip(actual, pred):\n",
    "            mat[i][j] += 1\n",
    "        return mat\n",
    "\n",
    "    def preprocess_data(self, x, y, no_classes, limit=None):\n",
    "        x = x[:limit]\n",
    "        y = y[:limit]\n",
    "        x = x.reshape(len(x), 3, 28, 28)\n",
    "        x = x.astype(\"float32\") / 255\n",
    "        y = np_utils.to_categorical(y)\n",
    "        y = y.reshape(len(y), no_classes, 1)\n",
    "        return x, y\n",
    "\n",
    "    def bb_intersection_over_union(self, boxA, boxB):\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "        boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "        boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "        iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "        return np.abs(iou)\n",
    "    def getMetrics(self, y_pred, actual):\n",
    "            \n",
    "        MoU = 0\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            MoU += self.bb_intersection_over_union(y_pred[i], actual[i])\n",
    "            \n",
    "        MSE = np.mean(0.5 * (actual - y_pred)**2)\n",
    "        MAE = np.mean(np.abs(actual - y_pred))\n",
    "\n",
    "        return MSE,MoU/y_pred.shape[0],MAE\n",
    "\n",
    "\n",
    "    def experiments(self, dataset):\n",
    "        dataset = Q2DataLoader()\n",
    "        X_train, Y_train, X_test, Y_test, X_val, Y_val = dataset.train_test_split()\n",
    "\n",
    "        no_classes = 8\n",
    "        x_train, y_train = self.preprocess_data(X_train, Y_train, no_classes)\n",
    "        x_test, y_test = self.preprocess_data(X_test, Y_test, no_classes)\n",
    "        x_val, y_val = self.preprocess_data(X_val, Y_val, no_classes)\n",
    "        \n",
    "        depth = 3\n",
    "        img_size = 28\n",
    "        kernel_size = 3\n",
    "        l1_size = img_size - kernel_size + 1\n",
    "        l2_size = l1_size - kernel_size + 1\n",
    "        l3_size = l2_size - kernel_size + 1\n",
    "        l4_size = l3_size - kernel_size + 1\n",
    "        no_class = 8\n",
    "\n",
    "        lr = 0.0001\n",
    "\n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "            Softmax()\n",
    "        ]\n",
    "        cnn = CNN()\n",
    "        th = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "            Softmax()\n",
    "        ]\n",
    "        sig = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "            Softmax()\n",
    "        ]\n",
    "        lrelu = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "\n",
    "        self.plot_error(lrelu, th, sig, 'MAX POOLING')\n",
    "\n",
    "        size = 3000\n",
    "\n",
    "        op = []\n",
    "        for x in x_test[:size]:\n",
    "            op.append(cnn.predict( network, x))\n",
    "\n",
    "        true_1hot = np.argmax(y_test[:size], axis = 1).reshape((size,))\n",
    "        pred_1hot = np.argmax(op, axis = 1).reshape((size,))\n",
    "        metr = self.get_metrics(pred_1hot, true_1hot)\n",
    "\n",
    "        print('acc:', list(metr[0]))\n",
    "        print('F1:', list(metr[1]))\n",
    "        print('AUC:', list(metr[2]),'\\n')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "            Softmax()\n",
    "        ]\n",
    "        th = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "            Softmax()\n",
    "        ]\n",
    "        sig = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "            Softmax()\n",
    "        ]\n",
    "        lrelu = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "\n",
    "        self.plot_error(lrelu, th, sig, 'AVG POOLING')\n",
    "\n",
    "        size = 3000\n",
    "\n",
    "        op = []\n",
    "        for x in x_test[:size]:\n",
    "            op.append(cnn.predict( network, x))\n",
    "\n",
    "        true_1hot = np.argmax(y_test[:size], axis = 1).reshape((size,))\n",
    "        pred_1hot = np.argmax(op, axis = 1).reshape((size,))\n",
    "        metr = self.get_metrics(pred_1hot, true_1hot)\n",
    "\n",
    "        print('acc:', list(metr[0]))\n",
    "        print('F1:', list(metr[1]))\n",
    "        print('AUC:', list(metr[2]),'\\n')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        dataset = Q1DataLoader()\n",
    "        X_train, Y_train, X_test, Y_test, X_val, Y_val = dataset.train_test_split()\n",
    "\n",
    "        no_classes = 2\n",
    "        x_train, y_train = self.preprocess_data(X_train, Y_train, no_classes)\n",
    "        x_test, y_test = self.preprocess_data(X_test, Y_test, no_classes)\n",
    "        x_val, y_val = self.preprocess_data(X_val, Y_val, no_classes)\n",
    "        \n",
    "        depth = 1\n",
    "        img_size = 28\n",
    "        kernel_size = 3\n",
    "        l1_size = img_size - kernel_size + 1\n",
    "        l2_size = l1_size - kernel_size + 1\n",
    "        l3_size = l2_size - kernel_size + 1\n",
    "        l4_size = l3_size - kernel_size + 1\n",
    "        no_class = 2\n",
    "\n",
    "        lr = 0.0001\n",
    "\n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "            \n",
    "        ]\n",
    "        cnn = CNN()\n",
    "        thm = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        thb = cnn.train(network, binary_cross_entropy, binary_cross_entropy_prime, x_train, y_train, epochs=20, learning_rate=lr)\n",
    "\n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        sigm = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        sigb = cnn.train(network, binary_cross_entropy, binary_cross_entropy_prime, x_train, y_train, epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        lrelum = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        lrelub = cnn.train(network, binary_cross_entropy, binary_cross_entropy_prime, x_train, y_train, epochs=20, learning_rate=lr)\n",
    "\n",
    "        # self.plot_error(lrelu, th, sig, 'MAX POOLING')\n",
    "        # lrelum = cnn.train(network, mse, mse_prime, x_train, y_train , epochs=20, learning_rate=lr)\n",
    "        self.plot_error(lrelum, thm, sigm, 'MSE MAX POOLING')\n",
    "        self.plot_error(lrelub, thb, sigb, 'BCE MAX POOLING')\n",
    "\n",
    "        size = 3000\n",
    "\n",
    "        op = []\n",
    "        for x in x_test[:size]:\n",
    "            op.append(cnn.predict( network, x))\n",
    "\n",
    "        true_1hot = np.argmax(y_test[:size], axis = 1).reshape((size,))\n",
    "        pred_1hot = np.argmax(op, axis = 1).reshape((size,))\n",
    "        metr = self.get_metrics(pred_1hot, true_1hot)\n",
    "\n",
    "        print('acc:', list(metr[0]))\n",
    "        print('F1:', list(metr[1]))\n",
    "        print('AUC:', list(metr[2]),'\\n')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        thm = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        thb = cnn.train(network, binary_cross_entropy, binary_cross_entropy_prime, x_train, y_train, epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        sigm = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        sigb = cnn.train(network, binary_cross_entropy, binary_cross_entropy_prime, x_train, y_train, epochs=20, learning_rate=lr)\n",
    "\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        lrelum = cnn.train(network, mse, mse_prime, x_train, y_train , epochs=20, learning_rate=lr)\n",
    "        lrelub = cnn.train(network, binary_cross_entropy, binary_cross_entropy_prime, x_train, y_train, epochs=20, learning_rate=lr)\n",
    "        self.plot_error(lrelum, thm, sigm, 'MSE AVG POOLING')\n",
    "        self.plot_error(lrelub, thb, sigb, 'BCE AVG POOLING')\n",
    "\n",
    "        size = 3000\n",
    "\n",
    "        op = []\n",
    "        for x in x_test[:size]:\n",
    "            op.append(cnn.predict( network, x))\n",
    "\n",
    "        true_1hot = np.argmax(y_test[:size], axis = 1).reshape((size,))\n",
    "        pred_1hot = np.argmax(op, axis = 1).reshape((size,))\n",
    "        metr = self.get_metrics(pred_1hot, true_1hot)\n",
    "\n",
    "        print('acc:', list(metr[0]))\n",
    "        print('F1:', list(metr[1]))\n",
    "        print('AUC:', list(metr[2]),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''TRAFFIC DATASET'''\n",
    "        ann_path = '/content/drive/MyDrive/IISc/PRNN/traffic_data/annotations'\n",
    "        image_path = '/content/drive/MyDrive/IISc/PRNN/traffic_data/images'\n",
    "        dataset = TrafficDataLoader(ann_path, image_path)\n",
    "        X_train, X_test, Y_train, Y_test = dataset.split()\n",
    "\n",
    "        no_classes = 4\n",
    "        x_train, y_train = self.preprocess_data(X_train, Y_train, no_classes)\n",
    "        x_test, y_test = self.preprocess_data(X_test, Y_test, no_classes)\n",
    "        \n",
    "        depth = 1\n",
    "        img_size = 100\n",
    "        kernel_size = 3\n",
    "        l1_size = img_size - kernel_size + 1\n",
    "        l2_size = l1_size - kernel_size + 1\n",
    "        l3_size = l2_size - kernel_size + 1\n",
    "        l4_size = l3_size - kernel_size + 1\n",
    "        no_class = 4\n",
    "\n",
    "        lr = 0.0001\n",
    "\n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        cnn = CNN()\n",
    "        th = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        sig = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            # Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            # Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        lrelu = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "\n",
    "        self.plot_error(lrelu, th, sig, 'MAX POOLING')\n",
    "\n",
    "\n",
    "        op = []\n",
    "        for x in x_test:\n",
    "            op.append(predict( network, x))\n",
    "        metr = self.getMetrics(np.array(op).reshape(264, 4)*100, y_test.reshape(264, 4)*100)\n",
    "        print(metr)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        th = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            # LkRelu(0.1),\n",
    "            Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        sig = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "        \n",
    "        network = [\n",
    "            Convolutional((3, 28, 28), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l1_size, l1_size), kernel_size),\n",
    "            Avgpool((depth, l1_size, l1_size), kernel_size),\n",
    "\n",
    "            Convolutional((3, 24, 24), kernel_size),\n",
    "            LkRelu(0.1),\n",
    "            # Sigmoid(),\n",
    "            # Tanh(),\n",
    "            # Maxpool((depth, l3_size, l3_size), kernel_size),\n",
    "            Avgpool((depth, l3_size, l3_size), kernel_size),\n",
    "            \n",
    "            Reshape((depth, l4_size, l4_size), (depth * l4_size * l4_size, 1)),\n",
    "            Dense(depth * l4_size * l4_size, no_class),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "        lrelu = cnn.train(network, mse, mse_prime, x_train , y_train , epochs=20, learning_rate=lr)\n",
    "\n",
    "        self.plot_error(lrelu, th, sig, 'AVG POOLING')\n",
    "\n",
    "        size = 3000\n",
    "\n",
    "\n",
    "        op = []\n",
    "        for x in x_test:\n",
    "            op.append(predict( network, x))\n",
    "        metr = self.getMetrics(np.array(op).reshape(264, 4)*100, y_test.reshape(264, 4)*100)\n",
    "        print(metr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ySTvZLLFH3",
   "metadata": {
    "id": "35ySTvZLLFH3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable \n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.2):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.linear1 = nn.Linear(hidden_dim, output_size)\n",
    "        # self.linear2 = nn.Linear(64, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        #x = x.long()\n",
    "        #embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.linear1(out)\n",
    "        # out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #print(weight)\n",
    "        hidden = ((weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()).to(torch.float32),\n",
    "                      (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()).to(torch.float32))\n",
    "        #print(hidden)\n",
    "        return hidden\n",
    "\n",
    "class LSTM_experiment():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def lstm_pytorch(self,dataset):\n",
    "      Xtrain, Ytrain, Xtest, Ytest, Xval, yval = dataset.train_test_split()\n",
    "\n",
    "      padding_size=np.shspe(Xtrain)[2]  \n",
    "      X_train = torch.from_numpy(Xtrain).float()\n",
    "      Y_train = torch.from_numpy(Ytrain).float()\n",
    "\n",
    "      X_test = torch.from_numpy(Xtest).float()\n",
    "      Y_test = torch.from_numpy(Ytest).float()\n",
    "\n",
    "      train_data = TensorDataset(X_train, Y_train)\n",
    "      test_data = TensorDataset(X_test, Y_test)\n",
    "\n",
    "      batch_size = 20\n",
    "\n",
    "      train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "      test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "      output_size = 1\n",
    "\n",
    "      #embedding_dim = 400\n",
    "      hidden_dim = 64\n",
    "      n_layers = 4\n",
    "      input_size = 15\n",
    "\n",
    "      model = Predictor(input_size, output_size, hidden_dim, n_layers)\n",
    "      #model.to(device)\n",
    "\n",
    "      lr=0.002\n",
    "      #criterion = nn.BCELoss()\n",
    "      criterion = nn.MSELoss()\n",
    "      # criterion=nn.CrossEntropyLoss()\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=3e-2)\n",
    "\n",
    "      epochs = 20\n",
    "      counter = 0\n",
    "      print_every = 1000\n",
    "      clip = 5\n",
    "      valid_loss_min = np.Inf\n",
    "      losses = []\n",
    "      model.train()\n",
    "      accuracy_list=[]\n",
    "      for i in range(epochs):\n",
    "          h = model.init_hidden(batch_size)\n",
    "          # h[0] = h[0].to(torch.float32)\n",
    "          for inputs, labels in train_loader:\n",
    "              counter += 1\n",
    "              h = tuple([e.data for e in h])\n",
    "              #inputs = inputs.to(torch.float64)\n",
    "              # print(h)\n",
    "              # print(len(h))\n",
    "              # inputs, labels = inputs, labels.to(device)\n",
    "\n",
    "              model.zero_grad()\n",
    "              output, h = model(inputs, h)\n",
    "              labels = labels.squeeze(-1)\n",
    "\n",
    "              # print(output)\n",
    "              loss = criterion(output, labels)\n",
    "              loss.backward()\n",
    "              nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "              optimizer.step()\n",
    "              \n",
    "              if counter%print_every == 0:\n",
    "                  val_h = model.init_hidden(batch_size)\n",
    "                  val_losses = []\n",
    "                  num_correct = 0\n",
    "                  num_yess = 0 \n",
    "                  model.eval()\n",
    "                  for inp, lab in test_loader:\n",
    "                      val_h = tuple([each.data for each in val_h])\n",
    "                      # inp, lab = inp.to(device), lab.to(device)\n",
    "                      #print(val_h[0].size)\n",
    "                      out, val_h = model(inp, val_h)\n",
    "                      # lab = lab.squeeze(-1)\n",
    "                      val_loss = criterion(out, lab.float())\n",
    "                      #val_loss = criterion(out.squeeze(), lab.float())\n",
    "                      val_losses.append(val_loss.item())\n",
    "                      pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "                      \n",
    "                      correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "                      correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "                      # pred = pred.squeeze(-1)\n",
    "                      temp=pred.detach()\n",
    "                      # print(lab,temp)\n",
    "                      if ((lab.numpy()==temp.numpy()).all()):\n",
    "                        yess = 1\n",
    "                        num_yess += 1\n",
    "                      else:\n",
    "                        yess = 0\n",
    "                      num_correct += np.sum(correct)\n",
    "                      \n",
    "                  model.train()\n",
    "                  print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                        \"Step: {}...\".format(counter),\n",
    "                        \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                        \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "                  test_acc = num_correct/len(test_loader.dataset)\n",
    "                  print(\"Test accuracy: {:.3f}%\".format(test_acc*100))\n",
    "                  accuracy_list.append(test_acc*100)\n",
    "                  if np.mean(val_losses) <= valid_loss_min:\n",
    "                      torch.save(model.state_dict(), './state_dict.pt')\n",
    "                      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                      valid_loss_min = np.mean(val_losses)\n",
    "        \n",
    "      f = plt.figure()\n",
    "      plt.plot(accuracy_list)\n",
    "      plt.xlabel(\"Epoch\")\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    def experiments(self,dataset):\n",
    "      X_train, Y_train, Xtest, Ytest, Xval, yval = dataset.train_test_split()\n",
    "\n",
    "      padding_size=np.shspe(X_train)[2]    \n",
    "\n",
    "      # X_train, Xval, Y_train, yval = train_test_split(Xtrain, Ytrain, test_size=0.1, random_state=123)\n",
    "      print(type(X_train))\n",
    "\n",
    "      X_train=tf.convert_to_tensor(X_train)\n",
    "      Y_train=tf.convert_to_tensor(Y_train)\n",
    "\n",
    "      X_val=tf.convert_to_tensor(Xval)\n",
    "      Y_val=tf.convert_to_tensor(yval)\n",
    "\n",
    "      X_test=tf.convert_to_tensor(Xtest)\n",
    "      Y_test=tf.convert_to_tensor(Ytest)\n",
    "\n",
    "      \n",
    "      input_shape=(20,padding_size)\n",
    "      model = keras.Sequential()\n",
    "      model.add(LSTM(128,input_shape=input_shape))\n",
    "      model.add(Dropout(0.2))\n",
    "      model.add(Dense(128, activation='relu'))\n",
    "      model.add(Dense(64, activation='relu'))\n",
    "      model.add(Dropout(0.4))\n",
    "      model.add(Dense(48, activation='relu'))\n",
    "      model.add(Dropout(0.4))\n",
    "      model.add(Dense(24, activation='softmax'))\n",
    "      model.summary()\n",
    "      model.compile(optimizer='adam',loss='SparseCategoricalCrossentropy',metrics=['acc'])\n",
    "\n",
    "      history = model.fit(X_train, Y_train, epochs=50, batch_size=16, validation_data=(Xval, Y_val), shuffle=False)\n",
    "\n",
    "      history_dict=history.history\n",
    "      loss_values=history_dict['loss']\n",
    "      acc_values=history_dict['acc']\n",
    "      val_loss_values = history_dict['val_loss']\n",
    "      val_acc_values=history_dict['val_acc']\n",
    "      epochs=range(1,51)\n",
    "      fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,10))\n",
    "      ax1.plot(epochs,loss_values,label='Training Loss')\n",
    "      ax1.plot(epochs,val_loss_values, label='Validation Loss')\n",
    "      ax1.set_title('Training and validation loss')\n",
    "      ax1.set_xlabel('Epochs')\n",
    "      ax1.set_ylabel('Loss')\n",
    "      ax1.legend()\n",
    "      ax2.plot(epochs,acc_values, label='Training accuracy')\n",
    "      ax2.plot(epochs,val_acc_values,label='Validation accuracy')\n",
    "      ax2.set_title('Training and validation accuracy')\n",
    "      ax2.set_xlabel('Epochs')\n",
    "      ax2.set_ylabel('Accuracy')\n",
    "      ax2.legend()\n",
    "      plt.show()\n",
    "\n",
    "      TrainLoss, Trainacc = model.evaluate(X_train,Y_train)\n",
    "      TestLoss, Testacc = model.evaluate(X_test, Y_test)\n",
    "      y_pred=model.predict(X_test)\n",
    "\n",
    "      print('Confusion_matrix: ',tf.math.confusion_matrix(Y_test, np.argmax(y_pred,axis=1)))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bea49f",
   "metadata": {
    "id": "d3bea49f",
    "outputId": "70379d48-f890-4503-828b-064cd5b4e67e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/208 [00:00<?, ?it/s]C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_9376\\2614723586.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  self.next_x = 1. / (1. + np.exp(-x))\n",
      "  0%|                                                                                 | 1/208 [00:01<03:52,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "0.5489054209279931 0.7421410365335599 0.6493750472315728 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                             | 51/208 [00:54<02:43,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 0]\n",
      "0.27969717930593463 0.9207731520815633 0.44525289934853346 0.8221153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|                                         | 101/208 [01:46<02:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 0 0 1]\n",
      "0.20263300558560837 0.9364910790144435 0.40835234038782986 0.8285256410256411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|                      | 151/208 [02:41<01:09,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 1 1]\n",
      "0.17267491530030493 0.9420135938827527 0.4364124281873167 0.8365384615384616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|  | 201/208 [03:34<00:04,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 1]\n",
      "0.15187883219710063 0.9430756159728122 0.41663965611293247 0.8557692307692307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 208/208 [03:39<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline metrics on test data= (0.8509615384615384, 0.8917345750873108, 0.8072649572649573)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 1/208 [00:00<01:56,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 1 1 1]\n",
      "0.3671343718773593 0.9182242990654206 0.4612694318644604 0.8301282051282052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                             | 51/208 [00:28<01:32,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 1]\n",
      "0.10112044258356785 0.9683517417162277 0.4387186270579116 0.8557692307692307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|                                                        | 62/208 [00:34<01:17,  1.88it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    dataset = Q1DataLoader()\n",
    "    mlp_experiments = MLP_Experiments()\n",
    "    mlp_experiments.experiments(dataset)\n",
    "    Standard_CNN_Models().experiments(dataset,1,2)\n",
    "    svm_experiments = SVM_Experiments()\n",
    "    svm_experiments.experiments(dataset)\n",
    "    \n",
    "    dataset = Q2DataLoader()\n",
    "    mlp_experiments = MLP_Experiments()\n",
    "    mlp_experiments.experiments(dataset)\n",
    "    Standard_CNN_Models().experiments(dataset,3,8)\n",
    "    \n",
    "    dataset = Q3DataLoader()\n",
    "    Standard_CNN_Models().experiments(dataset,3,4)\n",
    "    \n",
    "    dataset = Q4DataLoader()\n",
    "    mlp_experiments = MLP_Experiments()\n",
    "    mlp_experiments.experiments(dataset)\n",
    "\n",
    "    cnn_exp = CNN_Experiments()\n",
    "    cnn_exp.experiments()\n",
    "\n",
    "    dataset = Q4DataLoader()\n",
    "    lstm_experiments = LSTM_experiment()\n",
    "    lstm_experiments.experiments(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58aba1",
   "metadata": {
    "id": "4e58aba1"
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cd647",
   "metadata": {
    "id": "b23cd647"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb8f5f",
   "metadata": {
    "id": "1feb8f5f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bcc4d1",
   "metadata": {
    "id": "32bcc4d1"
   },
   "outputs": [],
   "source": [
    "print(np.zeros(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6b18b",
   "metadata": {
    "id": "2cb6b18b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Assignment2_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_name",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
